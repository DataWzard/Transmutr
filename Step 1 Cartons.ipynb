{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e7aea-3d51-4990-babc-2edb88a01f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas tqdm openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9528a8b-afeb-4677-97a4-c9da1bdb120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  17%|\u001b[34m███████████▏                                                       \u001b[0m| 1/6 [00:00<00:03,  1.63file/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 0.61 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  33%|\u001b[34m██████████████████████▎                                            \u001b[0m| 2/6 [00:01<00:02,  1.58file/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 0.65 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  50%|\u001b[34m█████████████████████████████████▌                                 \u001b[0m| 3/6 [00:01<00:01,  1.58file/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 0.63 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  67%|\u001b[34m████████████████████████████████████████████▋                      \u001b[0m| 4/6 [00:02<00:01,  1.43file/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 0.80 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  83%|\u001b[34m███████████████████████████████████████████████████████▊           \u001b[0m| 5/6 [00:03<00:00,  1.43file/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 0.69 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|\u001b[34m███████████████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:04<00:00,  1.47file/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 0.69 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Differences: 100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:00<00:00, 433.18file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.02 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:00<00:00, 386.42file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.02 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:00<00:00, 461.55file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.02 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:00<00:00, 600.10file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.01 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:00<00:00, 326.88file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.02 seconds to execute.\n",
      "Comparison Excel saved at Output Path\\Carton_Output_run_2441.xlsx\n",
      "Function 'process_files' took 502.59 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Decorator to measure execution time\n",
    "def time_complexity(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' took {end_time - start_time:.2f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Column mappings\n",
    "column_mapping = {\n",
    "    \"fulfillment_id\": \"orderId\",\n",
    "    \"ref_id\": \"refId\",\n",
    "    \"index\": \"index\",\n",
    "    \"name\": \"name\",\n",
    "    \"dimensions\": \"dimensions\",\n",
    "    \"cost\": \"Price\",\n",
    "    \"base_cost\": \"base_cost\",\n",
    "    \"total_volume\": \"Carton_volume\",\n",
    "    \"net_volume\": \"Order_volume\",\n",
    "    \"volume_utilization\": \"volume_utilization\",\n",
    "    \"surface_area\": \"surface_area\",\n",
    "    \"total_weight\": \"total_weight\",\n",
    "    \"net_weight\": \"net_weight\",\n",
    "    \"tare_weight\": \"tare_weight\",\n",
    "    \"weight_utilization\": \"weight_utilization\",\n",
    "    \"item_count\": \"item_count\",\n",
    "    \"dim_weight\": \"dim_weight\",\n",
    "}\n",
    "\n",
    "# Columns for consolidated output\n",
    "consolidated_columns = [\n",
    "    \"orderId\", \"refId\", \"index\", \"name\", \"dimensions\", \"Price\", \"base_cost\",\n",
    "    \"Carton_volume\", \"Order_volume\", \"volume_utilization\", \"surface_area\",\n",
    "    \"total_weight\", \"net_weight\", \"tare_weight\", \"weight_utilization\",\n",
    "    \"dim_weight\", \"item_count\", \"source_flag\"\n",
    "]\n",
    "\n",
    "# Combined sheet column order\n",
    "combined_columns = [\n",
    "    \"orderId_baseline\", \"orderId\", \"refId_baseline\", \"refId\",\n",
    "    \"index_baseline\", \"index\", \"name_baseline\", \"name\",\n",
    "    \"dimensions_baseline\", \"dimensions\", \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
    "    \"base_cost_baseline\", \"base_cost\", \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_Volume_Diff\",\n",
    "    \"Order_volume_baseline\", \"Order_volume\", \"Order_Volume_Diff\", \"volume_utilization_baseline\", \"volume_utilization\", \n",
    "    \"volume_utilization_Diff\", \"surface_area_baseline\", \"surface_area\", \"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\",\n",
    "    \"net_weight_baseline\", \"net_weight\", \"tare_weight_baseline\", \"tare_weight\",\n",
    "    \"weight_utilization_baseline\", \"weight_utilization\", \"dim_weight_baseline\", \"dim_weight\",\n",
    "    \"item_count_baseline\", \"item_count\", \"Item_Diff\"\n",
    "]\n",
    "\n",
    "# Columns to drop to save memory (only if truly necessary)\n",
    "columns_to_drop = [\"item_summary\"]  # Ensure this is correct; remove if you need it\n",
    "\n",
    "def refine_suffix(filename):\n",
    "    # Remove the file extension (e.g., .baseline, .opc_top-14)\n",
    "    name_without_extension = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Match and extract only the part after 'pacsimulate_'\n",
    "    match = re.search(r\"pacsimulate_(_+)\", name_without_extension)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the extracted portion\n",
    "    \n",
    "    # Fallback\n",
    "    return name_without_extension.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def clean_sheet_name(name):\n",
    "    \"\"\"Cleans up sheet names to ensure they are Excel-compatible.\"\"\"\n",
    "    cleaned_name = name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "    return cleaned_name[:31]\n",
    "\n",
    "@time_complexity\n",
    "def load_and_process_file_in_chunks(file_path, suffix, chunk_size=100000):\n",
    "    \"\"\"Loads a file in chunks, processes it, and returns a DataFrame.\"\"\"\n",
    "    try:\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, delimiter='|', low_memory=False, memory_map=True,\n",
    "            on_bad_lines='skip', chunksize=chunk_size\n",
    "        ):\n",
    "            # Drop unnecessary columns\n",
    "            if columns_to_drop:\n",
    "                chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "            \n",
    "            # Rename columns based on mapping\n",
    "            chunk = chunk.rename(columns={col: column_mapping[col] for col in column_mapping if col in chunk.columns})\n",
    "            \n",
    "            # Add source_flag\n",
    "            chunk[\"source_flag\"] = suffix\n",
    "            \n",
    "            # Reindex to ensure all required columns are present\n",
    "            chunk = chunk.reindex(columns=consolidated_columns, fill_value=None)\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_consolidated_fields(df):\n",
    "\n",
    "     # Calculate Dimmed\n",
    "    df['Dimmed'] = np.where(df['dim_weight'] > df['total_weight'], 'Yes', 'No')\n",
    "\n",
    "    # Calculate Billed Weight\n",
    "    df['Billed_Weight'] = np.where(df['dim_weight'] > df['total_weight'], np.ceil(df['dim_weight']), np.ceil(df['total_weight'])).astype(int)\n",
    "    \n",
    "    # Split dimensions into L, W, H\n",
    "    dimensions_split = df['dimensions'].str.split(',', expand=True)\n",
    "    \n",
    "    # Validate that the split resulted in exactly three parts\n",
    "    if dimensions_split.shape[1] != 3:\n",
    "        print(\"Warning: 'dimensions' column does not split into exactly three parts (L,W,H). Filling with NaN.\")\n",
    "        dimensions_split = dimensions_split.reindex(columns=[0,1,2], fill_value=np.nan)\n",
    "\n",
    "    # Assign to new columns\n",
    "    df['L'] = pd.to_numeric(dimensions_split[0].str.strip(), errors='coerce')\n",
    "    df['W'] = pd.to_numeric(dimensions_split[1].str.strip(), errors='coerce')\n",
    "    df['H'] = pd.to_numeric(dimensions_split[2].str.strip(), errors='coerce')\n",
    "\n",
    "    # Calculate Surface Area (SA)\n",
    "    df['SA'] = 2 * (df['L'] * df['W'] + df['L'] * df['H'] + df['W'] * df['H']) + 2 * (df['W'] ** 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "@time_complexity\n",
    "def compute_differences(df):\n",
    "    try:\n",
    "        # Define the column pairs for differences\n",
    "        column_pairs = [\n",
    "            (\"Price_baseline\", \"Price\", \"Price_Diff\"),\n",
    "            (\"Order_volume_baseline\", \"Order_volume\", \"Order_Volume_Diff\"),\n",
    "            (\"item_count_baseline\", \"item_count\", \"Item_Diff\"),\n",
    "            (\"Carton_volume_baseline\", \"Carton_volume\", \"Carton_Volume_Diff\"),\n",
    "            (\"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\"),\n",
    "            (\"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\"),\n",
    "        ]\n",
    "\n",
    "        for col_baseline, col, col_diff in tqdm(\n",
    "            column_pairs, desc=\"Computing Differences\", unit=\"file\", colour=\"green\"\n",
    "        ):\n",
    "            if col_baseline in df.columns and col in df.columns:\n",
    "                df[col_baseline] = pd.to_numeric(df[col_baseline], errors=\"coerce\").fillna(0)\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "                df[col_diff] = df[col] - df[col_baseline]\n",
    "            else:\n",
    "                print(f\"Skipping difference calculation for {col_diff}: Missing {col_baseline} or {col}\")\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_differences: {e}\")\n",
    "        return df\n",
    "\n",
    "@time_complexity\n",
    "def process_files(directory, output_directory):\n",
    "    \"\"\"Processes all files in a directory and creates consolidated and comparison outputs.\"\"\"\n",
    "    files = os.listdir(directory)\n",
    "    data_frames = {}\n",
    "    consolidated_data = []\n",
    "    baseline_file = None\n",
    "\n",
    "    # Identify the baseline file\n",
    "    for filename in files:\n",
    "        if \"baseline\" in filename.lower():\n",
    "            baseline_file = filename\n",
    "            break\n",
    "\n",
    "    if not baseline_file:\n",
    "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
    "\n",
    "    # Process each file\n",
    "    with tqdm(total=len(files), desc=\"Loading files\", unit=\"file\", colour=\"blue\") as pbar:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            file_suffix = refine_suffix(filename)\n",
    "            sheet_name = clean_sheet_name(file_suffix)\n",
    "            df = load_and_process_file_in_chunks(file_path, file_suffix, chunk_size=100000)\n",
    "            if not df.empty:\n",
    "                data_frames[file_suffix] = df  # Store for later use\n",
    "                consolidated_data.append(df)    # Append processed data\n",
    "            else:\n",
    "                print(f\"Warning: No valid data in file {filename}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Consolidate data into a single DataFrame\n",
    "    if consolidated_data:\n",
    "        consolidated_output = pd.concat(consolidated_data, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")\n",
    "        consolidated_output = pd.DataFrame(columns=consolidated_columns)\n",
    "    try:\n",
    "        consolidated_output = calculate_consolidated_fields(consolidated_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during consolidated calculations: {e}\")\n",
    "\n",
    "    # Extract the run# from the baseline file name\n",
    "    run_match = re.search(r\"pacsimulate_(\\d+)\", baseline_file)\n",
    "    run_number = run_match.group(1)\n",
    "    \n",
    "    # Define the new output file name\n",
    "    comparison_filename = f\"Carton_Output_run_{run_number}.xlsx\"\n",
    "    comparison_path = os.path.join(output_directory, comparison_filename)\n",
    "    \n",
    "    # Save comparison outputs to Excel\n",
    "    baseline_df = data_frames.pop(refine_suffix(baseline_file))\n",
    "    with pd.ExcelWriter(comparison_path, engine='openpyxl') as writer:\n",
    "        for key, df in data_frames.items():\n",
    "            sheet_name = clean_sheet_name(key)\n",
    "    \n",
    "            # Align comparison DataFrame columns with combined_columns\n",
    "            comparison_df = df.reindex(columns=[col.replace(\"_baseline\", \"\") for col in combined_columns if \"_baseline\" not in col])\n",
    "    \n",
    "            # Combine baseline and comparison data\n",
    "            combined_df = pd.concat(\n",
    "                [baseline_df.add_suffix(\"_baseline\"), comparison_df],\n",
    "                axis=1\n",
    "            ).reindex(columns=combined_columns)\n",
    "    \n",
    "            # Compute differences\n",
    "            combined_df = compute_differences(combined_df)\n",
    "    \n",
    "            # Keep only the difference columns\n",
    "            difference_columns = [\n",
    "                \"Price_Diff\", \"Order_Volume_Diff\", \"Item_Diff\", \n",
    "                \"Carton_Volume_Diff\", \"volume_utilization_Diff\", \"total_weight_Diff\"\n",
    "            ]\n",
    "    \n",
    "            # Write the combined DataFrame to the corresponding sheet\n",
    "            combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "            # Apply formulas for Dimmed and Billed Weight\n",
    "            workbook = writer.book\n",
    "            sheet = writer.sheets[sheet_name]\n",
    "    \n",
    "            headers = [str(cell.value).strip() for cell in sheet[1]]\n",
    "            dim_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"dim_weight_\")), None)\n",
    "            total_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"total_weight_\")), None)\n",
    "    \n",
    "            if dim_weight_col_idx is None or total_weight_col_idx is None:\n",
    "                print(f\"Warning: Required columns for Dimmed and Billed Weight formulas not found in '{sheet_name}'. Headers: {headers}\")\n",
    "                continue\n",
    "    \n",
    "            last_column = len(headers)\n",
    "            dimmed_col_letter = get_column_letter(last_column + 1)\n",
    "            billed_weight_col_letter = get_column_letter(last_column + 2)\n",
    "    \n",
    "            sheet[f\"{dimmed_col_letter}1\"] = \"Dimmed\"\n",
    "            sheet[f\"{billed_weight_col_letter}1\"] = \"Billed_Weight\"\n",
    "    \n",
    "            for row in range(2, sheet.max_row + 1):\n",
    "                sheet[f\"{dimmed_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \\\"Yes\\\", \\\"No\\\")\"\n",
    "                )\n",
    "                sheet[f\"{billed_weight_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \"\n",
    "                    f\"ROUNDUP({get_column_letter(dim_weight_col_idx)}{row}, 0), ROUNDUP({get_column_letter(total_weight_col_idx)}{row}, 0))\"\n",
    "                )\n",
    "            \n",
    "            final_df = combined_df[difference_columns]\n",
    "            # Non-bold headers\n",
    "            for cell in sheet[1]:\n",
    "                cell.font = Font(bold=False)\n",
    "            \n",
    "            for col in sheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = col[0].column_letter  # Get the column letter (e.g., 'A', 'B')\n",
    "            \n",
    "                for cell in col:\n",
    "                    if cell.value:\n",
    "                        cell_length = len(str(cell.value))\n",
    "                        if cell_length > max_length:\n",
    "                            max_length = cell_length\n",
    "            \n",
    "                adjusted_width = max_length + 0.5  # Adding padding\n",
    "                sheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "    print(f\"Comparison Excel saved at {comparison_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = r\"Upload Path\"\n",
    "    output_directory = r\"Output Path\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    process_files(directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442fda17-ae63-4d53-9f13-af4b2e3ce40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
