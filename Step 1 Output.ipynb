{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cecc3867-79bd-467b-b0ad-680f1c244018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  17%|\u001b[34m███████████▏                                                       \u001b[0m| 1/6 [00:02<00:11,  2.36s/file]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 2.36 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  33%|\u001b[34m██████████████████████▎                                            \u001b[0m| 2/6 [00:04<00:09,  2.30s/file]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 2.26 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  50%|\u001b[34m█████████████████████████████████▌                                 \u001b[0m| 3/6 [00:06<00:06,  2.29s/file]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 2.28 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  67%|\u001b[34m████████████████████████████████████████████▋                      \u001b[0m| 4/6 [00:09<00:04,  2.27s/file]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 2.22 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  83%|\u001b[34m███████████████████████████████████████████████████████▊           \u001b[0m| 5/6 [00:11<00:02,  2.22s/file]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 2.15 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|\u001b[34m███████████████████████████████████████████████████████████████████\u001b[0m| 6/6 [00:13<00:00,  2.24s/file]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_and_process_file_in_chunks' took 2.17 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated output saved at: Output Path\\consolidated_output.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m███████████████████████████████████████████████████████████\u001b[0m| 7/7 [00:00<00:00, 24.43file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.29 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m███████████████████████████████████████████████████████████\u001b[0m| 7/7 [00:00<00:00, 46.82file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.15 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m███████████████████████████████████████████████████████████\u001b[0m| 7/7 [00:00<00:00, 40.81file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.17 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m███████████████████████████████████████████████████████████\u001b[0m| 7/7 [00:00<00:00, 38.88file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.18 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Differences: 100%|\u001b[32m███████████████████████████████████████████████████████████\u001b[0m| 7/7 [00:00<00:00, 41.17file/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_differences' took 0.17 seconds to execute.\n",
      "Comparison Excel saved at Output Path\\Summary_Output_run_2441.xlsx\n",
      "Function 'process_files' took 1271.31 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "def time_complexity(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' took {elapsed_time:.2f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "column_mapping = {\n",
    "    \"fulfillment_id\": \"orderId\",\n",
    "    \"value\": \"Price\",\n",
    "    \"optimized_cartons\": \"Carton_Count\",\n",
    "    \"total_volume\": \"Carton_volume\",\n",
    "    \"net_volume\": \"Order_volume\",\n",
    "    \"items_total\": \"items_total\",\n",
    "    \"items_leftover\": \"items_leftover\",\n",
    "    \"volume_utilization\": \"volume_utilization\",\n",
    "    \"total_weight\": \"total_weight\",\n",
    "    \"box_counts\": \"box_counts\",\n",
    "    \"surface_area\": \"surface_area\",\n",
    "    \"weight_utilization\": \"weight_utilization\",\n",
    "    \"net_weight\": \"net_weight\",\n",
    "    \"tare_weight\": \"tare_weight\",\n",
    "    \"dim_weight\": \"dim_weight\"\n",
    "}\n",
    "\n",
    "# Columns for consolidated output\n",
    "consolidated_columns = [\n",
    "    \"orderId\", \"Price\", \"Carton_Count\", \"Carton_volume\", \"Order_volume\",\n",
    "    \"items_total\", \"items_leftover\",\"volume_utilization\", \"total_weight\", \n",
    "    \"box_counts\", \"surface_area\", \"weight_utilization\", \"net_weight\",\n",
    "    \"tare_weight\", \"dim_weight\", \"source_flag\"\n",
    "]\n",
    "\n",
    "combined_columns = [\n",
    "    \"orderId_baseline\", \"orderId\", \n",
    "    \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
    "    \"Carton_Count_baseline\", \"Carton_Count\", \n",
    "    \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_Volume_Diff\",\n",
    "    \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\",\n",
    "    \"items_total_baseline\", \"items_total\", \n",
    "    \"items_leftover_baseline\", \"items_leftover\",\"items_leftover_Diff\",\n",
    "    \"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\", \n",
    "    \"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\",\n",
    "    \"box_counts_baseline\", \"box_counts\", \"box_counts_Diff\",\n",
    "    \"surface_area_baseline\", \"surface_area\", \n",
    "    \"weight_utilization_baseline\", \"weight_utilization\",\n",
    "    \"net_weight_baseline\", \"net_weight\", \"tare_weight_baseline\", \"tare_weight\",\n",
    "    \"dim_weight_baseline\", \"dim_weight\"\n",
    "]\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"all_packed\", \"dim_rated\", \"zone\", \"original_price\",\n",
    "    \"optimized_price\", \"box_summary\", \"total_time\", \"item_sets\", \"pack_request\", \"pack_response\"\n",
    "]\n",
    "\n",
    "# Refined suffix extraction: After \"pacsimulate_####_\", capture the rest of the string\n",
    "def refine_suffix(filename):\n",
    "    \"\"\"Extracts suffix between '_2441.' and next period/end\"\"\"\n",
    "    match = re.search(r\"pacsimulate_(\\d+)\\.(.*?)(?:$|\\.)\", filename)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    return None\n",
    "\n",
    "@time_complexity\n",
    "# Loads a file in chunks, processes it, and returns a DataFrame\n",
    "def load_and_process_file_in_chunks(file_path, chunk_size=100000):\n",
    "    try:\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, delimiter='|', low_memory=False, memory_map=True,\n",
    "            on_bad_lines='skip', chunksize=chunk_size\n",
    "        ):\n",
    "            # Drop unnecessary columns\n",
    "            if columns_to_drop:\n",
    "                chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "            # Rename columns based on mapping\n",
    "            chunk = chunk.rename(columns={col: column_mapping[col] for col in column_mapping if col in chunk.columns})\n",
    "\n",
    "            # Extract the source_flag using the filename suffix\n",
    "            suffix = refine_suffix(file_path)\n",
    "            if suffix:\n",
    "                chunk[\"source_flag\"] = suffix\n",
    "            else:\n",
    "                print(f\"Warning: No valid suffix in file {file_path}.\")\n",
    "\n",
    "            # Reindex to ensure all required columns are present\n",
    "            chunk = chunk.reindex(columns=consolidated_columns, fill_value=None)\n",
    "\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_consolidated_fields(df):\n",
    "    # Calculate Dimmed\n",
    "    df['Dimmed'] = np.where(df['dim_weight'] > df['total_weight'], 'Yes', 'No')\n",
    "\n",
    "    # Calculate Billed Weight\n",
    "    df['Billed_Weight'] = np.where(df['dim_weight'] > df['total_weight'], np.ceil(df['dim_weight']), np.ceil(df['total_weight'])).astype(int)\n",
    "    \n",
    "    # Billed Over Actual\n",
    "    df['Billed_over_Actual'] = np.where(df['Billed_Weight'] > df['total_weight'], 1, 0)\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "@time_complexity\n",
    "def compute_differences(df):\n",
    "    try:\n",
    "        # Define the column pairs for differences\n",
    "        column_pairs = [\n",
    "            (\"Price_baseline\", \"Price\", \"Price_Diff\"),\n",
    "            (\"Order_volume_baseline\", \"Order_volume\", \"Order_Volume_Diff\"),\n",
    "            (\"Carton_volume_baseline\", \"Carton_volume\", \"Carton_Volume_Diff\"),\n",
    "            (\"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\"),\n",
    "            (\"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\"),\n",
    "            (\"items_leftover_baseline\", \"items_leftover\",\"items_leftover_Diff\"),  \n",
    "            (\"box_counts_baseline\", \"box_counts\", \"box_counts_Diff\"),\n",
    "        ]\n",
    "\n",
    "        for col_baseline, col, col_diff in tqdm(\n",
    "            column_pairs, desc=\"Computing Differences\", unit=\"file\", colour=\"green\"\n",
    "        ):\n",
    "            if col_baseline in df.columns and col in df.columns:\n",
    "                df[col_baseline] = pd.to_numeric(df[col_baseline], errors=\"coerce\").fillna(0)\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "                df[col_diff] = df[col] - df[col_baseline]\n",
    "            else:\n",
    "                print(f\"Skipping difference calculation for {col_diff}: Missing {col_baseline} or {col}\")\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_differences: {e}\")\n",
    "        return df\n",
    "\n",
    "@time_complexity\n",
    "def process_files(directory, output_directory):\n",
    "    files = os.listdir(directory)\n",
    "    data_frames = {}\n",
    "    consolidated_data = []\n",
    "    baseline_file = None\n",
    "\n",
    "    # Identify the baseline file\n",
    "    for filename in files:\n",
    "        if \"baseline\" in filename.lower():\n",
    "            baseline_file = filename\n",
    "            break\n",
    "\n",
    "    if not baseline_file:\n",
    "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
    "\n",
    "    # Process files\n",
    "    with tqdm(total=len(files), desc=\"Loading files\", unit=\"file\", colour=\"blue\") as pbar:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = load_and_process_file_in_chunks(file_path, chunk_size=100000)\n",
    "            if not df.empty:\n",
    "                data_frames[filename] = df\n",
    "                consolidated_data.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: No valid data in file {filename}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Consolidate all data into a single DataFrame\n",
    "    if consolidated_data:\n",
    "        consolidated_output = pd.concat(consolidated_data, ignore_index=True)\n",
    "        try:\n",
    "            # Perform any calculations \n",
    "            consolidated_output = calculate_consolidated_fields(consolidated_output)\n",
    "            \n",
    "            # Save the consolidated output\n",
    "            consolidated_output_path = os.path.join(output_directory, \"consolidated_output.xlsx\")\n",
    "            with pd.ExcelWriter(consolidated_output_path, engine='openpyxl') as writer:\n",
    "                consolidated_output.to_excel(writer, sheet_name=\"Consolidated_Output\", index=False)\n",
    "\n",
    "                workbook = writer.book\n",
    "                sheet = writer.sheets[\"Consolidated_Output\"]\n",
    "                # Header formating \n",
    "                for cell in sheet[1]:\n",
    "                    cell.font = Font(bold=False)\n",
    "                # column formatting\n",
    "                for col in sheet.columns:\n",
    "                    column_letter = col[0].column_letter\n",
    "                    # Get the maximum length of the column header and its values\n",
    "                    header_length = len(str(col[0].value)) if col[0].value else 0\n",
    "                    # Calculate the column width\n",
    "                    column_width = max(header_length + 0.5, 13)\n",
    "                    # Set the column width\n",
    "                    sheet.column_dimensions[column_letter].width = column_width\n",
    "\n",
    "            print(f\"Consolidated output saved at: {consolidated_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during consolidated calculations or saving: {e}\")\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")\n",
    "\n",
    "    # Extract the run # from the baseline file name\n",
    "    run_match = re.search(r\"pacsimulate_(\\d+)\", baseline_file)\n",
    "    run_number = run_match.group(1)\n",
    "    \n",
    "    # Define the new output file name\n",
    "    comparison_filename = f\"Summary_Output_run_{run_number}.xlsx\"\n",
    "    comparison_path = os.path.join(output_directory, comparison_filename)\n",
    "    \n",
    "    # Save comparison outputs to Excel\n",
    "    baseline_df = data_frames.pop(baseline_file)\n",
    "    with pd.ExcelWriter(comparison_path, engine='openpyxl') as writer:\n",
    "        for key, df in data_frames.items():\n",
    "            sheet_name = refine_suffix(key)  # Using the refined filename as sheet name\n",
    "            if not sheet_name:\n",
    "                print(f\"Skipping sheet for file {key} due to invalid suffix.\")\n",
    "                continue  # Skip empty suffix\n",
    "            \n",
    "            # Align comparison DataFrame columns with combined_columns\n",
    "            comparison_df = df.reindex(columns=[col.replace(\"_baseline\", \"\") for col in combined_columns if \"_baseline\" not in col])\n",
    "\n",
    "            # Combine baseline and comparison data\n",
    "            combined_df = pd.concat(\n",
    "                [baseline_df.add_suffix(\"_baseline\"), comparison_df],\n",
    "                axis=1\n",
    "            ).reindex(columns=combined_columns)\n",
    "\n",
    "            # Compute differences\n",
    "            combined_df = compute_differences(combined_df)\n",
    "\n",
    "            # Keep the difference columns\n",
    "            difference_columns = [\n",
    "                \"Price_Diff\", \"Order_Volume_Diff\", \"Carton_Volume_Diff\",\n",
    "                \"volume_utilization_Diff\", \"total_weight_Diff\", \n",
    "                \"items_leftover_Diff\", \"box_counts_Diff\"\n",
    "            ]\n",
    "\n",
    "            if combined_df.empty:\n",
    "                print(f\"Warning: Sheet {sheet_name} has no data. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Write the combined DataFrame to the corresponding sheet\n",
    "            combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            # Apply formulas for Dimmed and Billed Weight\n",
    "            workbook = writer.book\n",
    "            sheet = writer.sheets[sheet_name]\n",
    "\n",
    "            headers = [str(cell.value).strip() for cell in sheet[1]]\n",
    "            dim_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"dim_weight_\")), None)\n",
    "            total_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"total_weight_\")), None)\n",
    "\n",
    "            if dim_weight_col_idx is None or total_weight_col_idx is None:\n",
    "                print(f\"Warning: Required columns for Dimmed and Billed Weight formulas not found in '{sheet_name}'. Headers: {headers}\")\n",
    "                continue\n",
    "\n",
    "            last_column = len(headers)\n",
    "            dimmed_col_letter = get_column_letter(last_column + 1)\n",
    "            billed_weight_col_letter = get_column_letter(last_column + 2)\n",
    "\n",
    "            sheet[f\"{dimmed_col_letter}1\"] = \"Dimmed\"\n",
    "            sheet[f\"{billed_weight_col_letter}1\"] = \"Billed_Weight\"\n",
    "\n",
    "            for row in range(2, sheet.max_row + 1):\n",
    "                sheet[f\"{dimmed_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \\\"Yes\\\", \\\"No\\\")\"\n",
    "                )\n",
    "                sheet[f\"{billed_weight_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \"\n",
    "                    f\"ROUNDUP({get_column_letter(dim_weight_col_idx)}{row}, 0), ROUNDUP({get_column_letter(total_weight_col_idx)}{row}, 0))\"\n",
    "                )\n",
    "\n",
    "            final_df = combined_df[difference_columns]\n",
    "            # Header formating \n",
    "            for cell in sheet[1]:\n",
    "                cell.font = Font(bold=False)\n",
    "            # column formatting\n",
    "            for col in sheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = col[0].column_letter  \n",
    "\n",
    "                for col in sheet.columns:\n",
    "                    column_letter = col[0].column_letter\n",
    "                   #  Get the maximum length of the column header and its values\n",
    "                    header_length = len(str(col[0].value)) if col[0].value else 0\n",
    "                    # Calculate the column width\n",
    "                    column_width = max(header_length + 0.5, 13)\n",
    "                    # Set the column width\n",
    "                    sheet.column_dimensions[column_letter].width = column_width\n",
    "\n",
    "    print(f\"Comparison Excel saved at {comparison_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = r\"Upload Path\"\n",
    "    output_directory = r\"Output Path\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    process_files(directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bde3a3-3bf8-4c5d-af30-db3304bc48af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
