{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transmutr\n",
        "\n",
        "**Transmutr** is a custom ETL (Extract, Transform, Load) project designed to streamline the processing of data files and generate comprehensive Excel reports. This repository contains scripts that efficiently handle large datasets by extracting data from multiple sources, transforming and cleaning the data, computing essential metrics, and loading the final output into structured Excel workbooks with detailed comparisons and calculated differences.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Efficient Data Handling:**  \n",
        "  Processes extensive CSV files in manageable chunks, optimizing memory usage and ensuring smooth performance even with large datasets.\n",
        "\n",
        "- **Smart Data Transformation:**  \n",
        "  Automatically renames columns based on predefined mappings, enriches data with calculated fields like *Dimmed* and *Billed_Weight*, and breaks down complex dimension strings into individual length, width, and height components.\n",
        "\n",
        "- **Comprehensive Data Consolidation:**  \n",
        "  Integrates data from various sources, identifies baseline datasets, and meticulously computes differences to highlight key variances between baseline and other datasets.\n",
        "\n",
        "- **Automated Excel Reporting:**  \n",
        "  Generates detailed Excel reports with multiple sheets, incorporating comparison data and dynamic formulas for essential metrics. The reports feature clear formatting, dynamic column widths, and consistent styling for easy analysis.\n",
        "\n",
        "- **Performance Monitoring:**  \n",
        "  Utilizes decorators to measure and display the execution time of critical functions, ensuring the ETL pipeline remains efficient and responsive.\n",
        "\n",
        "- **User-Friendly Configuration:**  \n",
        "  Offers straightforward setup with easily configurable input and output directories, and gracefully handles potential data inconsistencies to provide a seamless user experience.\n"
      ],
      "metadata": {
        "id": "Nu7eO4l1OpiO"
      },
      "id": "Nu7eO4l1OpiO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This block only needs to be run the first time to initialize the packages***"
      ],
      "metadata": {
        "id": "av13SQrnQ8tF"
      },
      "id": "av13SQrnQ8tF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282e7aea-3d51-4990-babc-2edb88a01f33",
      "metadata": {
        "id": "282e7aea-3d51-4990-babc-2edb88a01f33"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Here âœ…**\n",
        "\n",
        "*Click play button on each block to start script*\n",
        "\n",
        "*once each script has finished proceed to the next block*"
      ],
      "metadata": {
        "id": "vIDoNa3_Qs6b"
      },
      "id": "vIDoNa3_Qs6b"
    },
    {
      "cell_type": "code",
      "source": [
        "sim_id = \"2626\" #<--- Enter Sim ID or Run number here"
      ],
      "metadata": {
        "id": "q9yTYhn5kMtA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741638949669,
          "user_tz": 420,
          "elapsed": 438,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "q9yTYhn5kMtA",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from google.cloud import storage\n",
        "\n",
        "# Initialize GCS Client\n",
        "client = storage.Client()\n",
        "bucket_name = \"abacus-bucket\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "\n",
        "sim_result_id_pattern = re.compile(\n",
        "    rf'gcs_sim_id={sim_id}/gcs_sim_result_id=(\\d+)/'\n",
        ")\n",
        "blobs = list(bucket.list_blobs(prefix=f\"pacsimulate_simulations_demo/gcs_sim_id={sim_id}/\"))\n",
        "sim_result_ids = []\n",
        "\n",
        "for blob in blobs:\n",
        "    match = sim_result_id_pattern.search(blob.name)\n",
        "    if match:\n",
        "        sim_result_ids.append(int(match.group(1)))\n",
        "\n",
        "if sim_result_ids:\n",
        "    largest_sim_result_id = max(sim_result_ids)\n",
        "    print(f\"Largest sim_result_id for sim_id={sim_id}: {largest_sim_result_id}\")\n",
        "else:\n",
        "    raise ValueError(f\"No results found for sim_id={sim_id}\")\n",
        "\n",
        "# Filter the blobs by regex\n",
        "pattern = re.compile(\n",
        "    rf'^pacsimulate_simulations_demo/'\n",
        "    rf'gcs_sim_id={sim_id}/'\n",
        "    rf'gcs_sim_result_id={largest_sim_result_id}/'\n",
        "    rf'results_data/raw/'\n",
        "    rf'.*\\.(output|output_cartons)$'\n",
        ")\n",
        "\n",
        "# Directly filter blobs before download\n",
        "matching_blobs = [blob for blob in blobs if pattern.match(blob.name)]\n",
        "\n",
        "# Step 3: Download only filtered blobs\n",
        "def create_local_directory_from_blob(blob_name, base_directory=\"Upload_Path\"):\n",
        "    local_path = os.path.join(base_directory, *blob_name.split('/'))\n",
        "    local_directory = os.path.dirname(local_path)\n",
        "    os.makedirs(local_directory, exist_ok=True)\n",
        "    return local_path\n",
        "\n",
        "def download_filtered_blobs(filtered_blobs):\n",
        "    downloaded_files = [\"baseline\", \"opc\", \"\"]\n",
        "    exclude_files = [\"preflight_baseline\", \"perfect\", \"all-candidates\"]\n",
        "\n",
        "    for blob in filtered_blobs:\n",
        "        # Skip excluded files\n",
        "        if any(excluded in blob.name for excluded in exclude_files):\n",
        "            print(f\"Skipping: {blob.name}\")\n",
        "            continue\n",
        "\n",
        "        local_path = create_local_directory_from_blob(blob.name)\n",
        "        blob.download_to_filename(local_path)\n",
        "        downloaded_files.append(local_path)\n",
        "        print(f\"Downloaded: {blob.name} -> {local_path}\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "\n",
        "# Step 4: Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # Download only the filtered blobs\n",
        "    if matching_blobs:\n",
        "        downloaded_files = download_filtered_blobs(matching_blobs)\n",
        "        output_directory = \"Output_Path\"\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "        print(f\"Files ready for processing in '{output_directory}'.\")\n",
        "    else:\n",
        "        print(\"No matching blobs found for download.\")\n"
      ],
      "metadata": {
        "id": "fNTQEcG2C6wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741638954211,
          "user_tz": 420,
          "elapsed": 2573,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5f01a949-5d7b-4431-d8cd-208c75f25da2"
      },
      "id": "fNTQEcG2C6wM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest sim_result_id for sim_id=2626: 3300\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output_cartons -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output_cartons\n",
            "Skipping: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.all-candidates.output\n",
            "Skipping: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.all-candidates.output_cartons\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.baseline.output -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.baseline.output\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.baseline.output_cartons -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.baseline.output_cartons\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-5.output -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-5.output\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-5.output_cartons -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-5.output_cartons\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-6.output -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-6.output\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-6.output_cartons -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-6.output_cartons\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-7.output -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-7.output\n",
            "Downloaded: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-7.output_cartons -> Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.opc_top-7.output_cartons\n",
            "Skipping: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.perfect.output\n",
            "Skipping: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.perfect.output_cartons\n",
            "Skipping: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.preflight_baseline.output\n",
            "Skipping: pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.preflight_baseline.output_cartons\n",
            "Files ready for processing in 'Output_Path'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Step ðŸ™ŒðŸ™ŒðŸ™Œ"
      ],
      "metadata": {
        "id": "S4uG8vYNRgHO"
      },
      "id": "S4uG8vYNRgHO"
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font\n",
        "from openpyxl.styles import NamedStyle\n",
        "\n",
        "# Decorator to measure execution time\n",
        "def time_complexity(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' took {end_time - start_time:.2f} seconds to execute.\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Column rename mapping\n",
        "column_mapping = {\n",
        "    \"fulfillment_id\": \"orderId\",\n",
        "    \"ref_id\": \"refId\",\n",
        "    \"index\": \"index\",\n",
        "    \"name\": \"name\",\n",
        "    \"dimensions\": \"dimensions\",\n",
        "    \"cost\": \"Price\",\n",
        "    \"base_cost\": \"base_cost\",\n",
        "    \"total_volume\": \"Carton_volume\",\n",
        "    \"net_volume\": \"Order_volume\",\n",
        "    \"volume_utilization\": \"volume_utilization\",\n",
        "    \"surface_area\": \"surface_area\",\n",
        "    \"total_weight\": \"total_weight\",\n",
        "    \"net_weight\": \"net_weight\",\n",
        "    \"tare_weight\": \"tare_weight\",\n",
        "    \"weight_utilization\": \"weight_utilization\",\n",
        "    \"item_count\": \"item_count\",\n",
        "    \"dim_weight\": \"dim_weight\",\n",
        "}\n",
        "\n",
        "# Columns for consolidated output\n",
        "consolidated_columns = [\n",
        "    \"orderId\", \"refId\", \"index\", \"name\", \"dimensions\", \"Price\", \"base_cost\",\n",
        "    \"Carton_volume\", \"Order_volume\", \"volume_utilization\", \"surface_area\",\n",
        "    \"total_weight\", \"net_weight\", \"tare_weight\", \"weight_utilization\",\n",
        "    \"dim_weight\", \"item_count\", \"source_flag\"\n",
        "]\n",
        "\n",
        "# Combined sheet column order\n",
        "combined_columns = [\n",
        "    \"orderId_baseline\", \"orderId\", \"refId_baseline\", \"refId\",\n",
        "    \"index_baseline\", \"index\", \"name_baseline\", \"name\",\n",
        "    \"dimensions_baseline\", \"dimensions\", \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
        "    \"base_cost_baseline\", \"base_cost\", \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\",\n",
        "    \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\", \"volume_utilization_baseline\", \"volume_utilization\",\n",
        "    \"volume_utilization_Diff\", \"surface_area_baseline\", \"surface_area\", \"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\",\n",
        "    \"net_weight_baseline\", \"net_weight\", \"tare_weight_baseline\", \"tare_weight\",\n",
        "    \"weight_utilization_baseline\", \"weight_utilization\", \"dim_weight_baseline\", \"dim_weight\",\n",
        "    \"item_count_baseline\", \"item_count\", \"Item_Diff\"\n",
        "]\n",
        "\n",
        " # Columns to drop to save memory\n",
        "columns_to_drop = [\"item_summary\"]\n",
        "\n",
        "sim_result_id_pattern = re.compile(\n",
        "    rf'gcs_sim_id={sim_id}/gcs_sim_result_id=(\\d+)/'\n",
        ")\n",
        "blobs = list(bucket.list_blobs(prefix=f\"pacsimulate_simulations_demo/gcs_sim_id={sim_id}/\"))\n",
        "sim_result_ids = []\n",
        "\n",
        "for blob in blobs:\n",
        "    match = sim_result_id_pattern.search(blob.name)\n",
        "    if match:\n",
        "        sim_result_ids.append(int(match.group(1)))\n",
        "\n",
        "if sim_result_ids:\n",
        "    largest_sim_result_id = max(sim_result_ids)\n",
        "    print(f\"Largest sim_result_id for sim_id={sim_id}: {largest_sim_result_id}\")\n",
        "else:\n",
        "    raise ValueError(f\"No results found for sim_id={sim_id}\")\n",
        "\n",
        "# Refined suffix extraction: After \"pacsimulate_####_\", capture the rest of the string\n",
        "def refine_suffix(filename):\n",
        "    \"\"\"Extracts suffix between '_2441.' and next period/end\"\"\"\n",
        "    match = re.search(r\"pacsimulate_(\\d+)\\.(.*?)(?:$|\\.)\", filename)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "    return None\n",
        "\n",
        "@time_complexity\n",
        "# Loads a file in chunks, processes it, and returns a DataFrame\n",
        "def load_and_process_file_in_chunks(file_path, chunk_size=100000):\n",
        "    try:\n",
        "        chunks = []\n",
        "        for chunk in pd.read_csv(\n",
        "            file_path, delimiter='|', low_memory=False, memory_map=True,\n",
        "            on_bad_lines='skip', chunksize=chunk_size\n",
        "        ):\n",
        "            # Drop unnecessary columns\n",
        "            if columns_to_drop:\n",
        "                chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "            # Rename columns based on mapping\n",
        "            chunk = chunk.rename(columns={col: column_mapping[col] for col in column_mapping if col in chunk.columns})\n",
        "\n",
        "            # Extract the source_flag using the filename suffix\n",
        "            suffix = refine_suffix(file_path)\n",
        "            if suffix:\n",
        "                chunk[\"source_flag\"] = suffix\n",
        "            else:\n",
        "                print(f\"Warning: No valid suffix in file {file_path}.\")\n",
        "\n",
        "            # Reindex to ensure all required columns are present\n",
        "            chunk = chunk.reindex(columns=consolidated_columns, fill_value=None)\n",
        "\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return pd.concat(chunks, ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load file {file_path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def calculate_consolidated_fields(df):\n",
        "    # Calculate Dimmed\n",
        "    df['Dimmed'] = np.where(df['dim_weight'] > df['total_weight'], 'Yes', 'No')\n",
        "\n",
        "    # Calculate Billed Weight\n",
        "    df['Billed_Weight'] = np.where(df['dim_weight'] > df['total_weight'], np.ceil(df['dim_weight']), np.ceil(df['total_weight'])).astype(int)\n",
        "\n",
        "    # Billed Over Actual\n",
        "    df['total_weight'] = np.ceil(df['total_weight'])\n",
        "\n",
        "    # Then calculate the 'Billed_over_Actual' column\n",
        "    df['Billed_over_Actual'] = np.where(df['Billed_Weight'] - df['total_weight'] > 0, df['Billed_Weight'] - df['total_weight'], 0)\n",
        "\n",
        "    # Split dimensions into L, W, H\n",
        "    dimensions_split = df['dimensions'].str.split(',', expand=True)\n",
        "\n",
        "    # Validate that the split resulted in exactly three parts\n",
        "    #if dimensions_split.shape[1] != 3:\n",
        "    #    print(\"Warning: 'dimensions' column does not split into exactly three parts (L,W,H). Filling with NaN.\")\n",
        "    #    dimensions_split = dimensions_split.reindex(columns=[0,1,2], fill_value=np.nan)\n",
        "\n",
        "    # Assign to new columns\n",
        "    df['L'] = pd.to_numeric(dimensions_split[0].str.strip(), errors='coerce')\n",
        "    df['W'] = pd.to_numeric(dimensions_split[1].str.strip(), errors='coerce')\n",
        "    df['H'] = pd.to_numeric(dimensions_split[2].str.strip(), errors='coerce')\n",
        "\n",
        "    # Calculate Surface Area (SA)\n",
        "    #df['SA'] = 2 * (df['L'] * df['W'] + df['L'] * df['H'] + df['W'] * df['H']) + 2 * (df['W'] ** 2)\n",
        "\n",
        "    return df\n",
        "\n",
        "@time_complexity\n",
        "def compute_differences(df):\n",
        "    try:\n",
        "        # Define the column pairs for differences\n",
        "        column_pairs = [\n",
        "            (\"Price_baseline\", \"Price\", \"Price_Diff\"),\n",
        "            (\"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\"),\n",
        "            (\"item_count_baseline\", \"item_count\", \"Item_Diff\"),\n",
        "            (\"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\"),\n",
        "            (\"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\"),\n",
        "            (\"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\"),\n",
        "        ]\n",
        "\n",
        "        for col_baseline, col, col_diff in tqdm(\n",
        "            column_pairs, desc=\"Computing Differences\", unit=\"file\", colour=\"green\"\n",
        "        ):\n",
        "            if col_baseline in df.columns and col in df.columns:\n",
        "                df[col_baseline] = pd.to_numeric(df[col_baseline], errors=\"coerce\").fillna(0)\n",
        "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
        "                df[col_diff] = df[col] - df[col_baseline]\n",
        "            else:\n",
        "                print(f\"Skipping difference calculation for {col_diff}: Missing {col_baseline} or {col}\")\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error in compute_differences: {e}\")\n",
        "        return df\n",
        "\n",
        "def apply_dynamic_column_formats(sheet):\n",
        "    # Define the number style with 2 decimal places for numeric columns\n",
        "    number_style = NamedStyle(name=\"number\")\n",
        "    number_style.number_format = '0.00'  # Number format for 2 decimal places\n",
        "\n",
        "    # Define the percentage style for utilization columns\n",
        "    percentage_style = NamedStyle(name=\"percentage\")\n",
        "    percentage_style.number_format = '0.00%'  # Percentage format for utilization columns\n",
        "\n",
        "    # List of columns to apply number formatting (Price, Carton Volume, etc.)\n",
        "    numeric_columns = [\n",
        "        \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
        "        \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\",\n",
        "        \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\",\n",
        "        \"surface_area_baseline\", \"surface_area\",\n",
        "        \"dim_weight_baseline\", \"dim_weight\"\n",
        "    ]\n",
        "\n",
        "    # List of columns for utilization percentage formatting\n",
        "    utilization_columns = [\n",
        "        \"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\",\n",
        "        \"weight_utilization_baseline\", \"weight_utilization\"\n",
        "    ]\n",
        "\n",
        "    # Loop through columns and apply styles based on column name\n",
        "    for col in sheet.columns:\n",
        "        column_letter = col[0].column_letter\n",
        "        column_name = str(col[0].value).strip() if col[0].value else \"\"\n",
        "\n",
        "        # Apply number format to numeric columns\n",
        "        if column_name in numeric_columns:\n",
        "            for cell in col:\n",
        "                cell.number_format = '0.00'  # Apply number format with 2 decimal places\n",
        "\n",
        "        # Apply percentage format to utilization columns\n",
        "        elif column_name in utilization_columns:\n",
        "            for cell in col:\n",
        "                cell.number_format = '0.00%'  # Apply percentage format\n",
        "@time_complexity\n",
        "def process_files(directory, output_directory):\n",
        "    files = os.walk(directory)\n",
        "    exclude_files = [\"preflight_baseline\", \"perfect\", \"all-candidates\"]  # ðŸš¨ Excluded files\n",
        "    data_frames = {}\n",
        "    consolidated_data = []\n",
        "    files = []\n",
        "\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            base_name = os.path.basename(filename)\n",
        "            if not any(excluded in base_name for excluded in exclude_files):  # âœ… Exclude unwanted files\n",
        "                files.append(os.path.relpath(os.path.join(root, filename), directory))  # âœ… Fix double \"Upload_Path\"\n",
        "\n",
        "    if not files:\n",
        "        print(\"No matching files found for processing.\")\n",
        "        return\n",
        "\n",
        "    # âœ… Ensure a valid baseline file exists\n",
        "    baseline_files = [f for f in files if \"baseline\" in os.path.basename(f).lower()]\n",
        "    if not baseline_files:\n",
        "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
        "    baseline_file = baseline_files[0]  # Take the first matching file\n",
        "\n",
        "    # Process files\n",
        "    with tqdm(total=len(files), desc=\"Loading files\", unit=\"file\", colour=\"blue\") as pbar:\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            df = load_and_process_file_in_chunks(file_path, chunk_size=100000)\n",
        "            if not df.empty:\n",
        "                price_columns = (\"Price_baseline\", \"Price\", \"Price_Diff\")\n",
        "                for col in price_columns:\n",
        "                    if col in df.columns:\n",
        "                        # Convert column to numeric , then multiply by 100\n",
        "                        df[col] = pd.to_numeric(df[col], errors='coerce') / 100\n",
        "                data_frames[filename] = df\n",
        "                consolidated_data.append(df)\n",
        "            else:\n",
        "                print(f\"Warning: No valid data in file {filename}\")\n",
        "            pbar.update(n=1)\n",
        "\n",
        "\n",
        "    # Consolidate all data into a single DataFrame\n",
        "    if consolidated_data:\n",
        "        consolidated_output = pd.concat(consolidated_data, ignore_index=True)\n",
        "        try:\n",
        "            # Perform any calculations\n",
        "            consolidated_output = calculate_consolidated_fields(consolidated_output)\n",
        "\n",
        "            # Save the consolidated output\n",
        "            consolidated_output_path = os.path.join(output_directory, \"consolidated_output.xlsx\")\n",
        "            with pd.ExcelWriter(consolidated_output_path, engine='openpyxl') as writer:\n",
        "                consolidated_output.to_excel(writer, sheet_name=\"Consolidated_Output\", index=False)\n",
        "\n",
        "                workbook = writer.book\n",
        "                sheet = writer.sheets[\"Consolidated_Output\"]\n",
        "\n",
        "\n",
        "                # Header formating\n",
        "                for cell in sheet[1]:\n",
        "                    cell.font = Font(bold=False)\n",
        "                # column formatting\n",
        "                for col in sheet.columns:\n",
        "                    column_letter = col[0].column_letter\n",
        "                    # Get the maximum length of the column header and its values\n",
        "                    header_length = len(str(col[0].value)) if col[0].value else 0\n",
        "                    # Calculate the column width\n",
        "                    column_width = max(header_length + 0.5, 13)\n",
        "                    # Set the column width\n",
        "                    sheet.column_dimensions[column_letter].width = column_width\n",
        "\n",
        "            print(f\"Consolidated output saved at: {consolidated_output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during consolidated calculations or saving: {e}\")\n",
        "    else:\n",
        "        print(\"No data to consolidate.\")\n",
        "\n",
        "    # Define the new output file name\n",
        "    comparison_filename = \"combined_output.xlsx\"\n",
        "    comparison_path = os.path.join(output_directory, comparison_filename)\n",
        "\n",
        "    # Save comparison outputs to Excel\n",
        "    baseline_df = data_frames.pop(baseline_file)\n",
        "    with pd.ExcelWriter(comparison_path, engine='openpyxl') as writer:\n",
        "        for key, df in data_frames.items():\n",
        "            sheet_name = refine_suffix(key)  # Using the refined filename as sheet name\n",
        "            if not sheet_name:\n",
        "                print(f\"Skipping sheet for file {key} due to invalid suffix.\")\n",
        "                continue  # Skip empty suffix\n",
        "\n",
        "            # Align comparison DataFrame columns with combined_columns\n",
        "            comparison_df = df.reindex(columns=[col.replace(\"_baseline\", \"\") for col in combined_columns if \"_baseline\" not in col])\n",
        "\n",
        "            # Combine baseline and comparison data\n",
        "            combined_df = pd.concat(\n",
        "                [baseline_df.add_suffix(\"_baseline\"), comparison_df],\n",
        "                axis=1\n",
        "            ).reindex(columns=combined_columns)\n",
        "\n",
        "            # Compute differences\n",
        "            combined_df = compute_differences(combined_df)\n",
        "\n",
        "            # Keep the difference columns\n",
        "            difference_columns = [\n",
        "                \"Price_Diff\", \"Order_volume_Diff\", \"Item_Diff\",\n",
        "                \"Carton_volume_Diff\", \"volume_utilization_Diff\", \"total_weight_Diff\"\n",
        "            ]\n",
        "\n",
        "            if combined_df.empty:\n",
        "                print(f\"Warning: Sheet {sheet_name} has no data. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Write the combined DataFrame to the corresponding sheet\n",
        "            combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "            # Apply formulas for Dimmed and Billed Weight\n",
        "            workbook = writer.book\n",
        "            sheet = writer.sheets[sheet_name]\n",
        "\n",
        "            # Apply dynamic formatting\n",
        "            apply_dynamic_column_formats(sheet)\n",
        "\n",
        "            # Header formating\n",
        "            for cell in sheet[1]:\n",
        "                cell.font = Font(bold=False)\n",
        "\n",
        "            # Column width adjustments\n",
        "            for col in sheet.columns:\n",
        "                column_letter = col[0].column_letter\n",
        "                header_length = len(str(col[0].value)) if col[0].value else 0\n",
        "                column_width = max(header_length + 0.5, 13)\n",
        "                sheet.column_dimensions[column_letter].width = column_width\n",
        "\n",
        "            headers = [str(cell.value).strip() for cell in sheet[1]]\n",
        "            dim_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"dim_weight_\")), None)\n",
        "            total_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"total_weight_\")), None)\n",
        "\n",
        "            if dim_weight_col_idx is None or total_weight_col_idx is None:\n",
        "                print(f\"Warning: Required columns for Dimmed and Billed Weight formulas not found in '{sheet_name}'. Headers: {headers}\")\n",
        "                continue\n",
        "\n",
        "            last_column = len(headers)\n",
        "            dimmed_col_letter = get_column_letter(last_column + 1)\n",
        "            billed_weight_col_letter = get_column_letter(last_column + 2)\n",
        "\n",
        "            sheet[f\"{dimmed_col_letter}1\"] = \"Dimmed\"\n",
        "            sheet[f\"{billed_weight_col_letter}1\"] = \"Billed_Weight\"\n",
        "\n",
        "            for row in range(2, sheet.max_row + 1):\n",
        "                sheet[f\"{dimmed_col_letter}{row}\"] = (\n",
        "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \\\"Yes\\\", \\\"No\\\")\"\n",
        "                )\n",
        "                sheet[f\"{billed_weight_col_letter}{row}\"] = (\n",
        "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \"\n",
        "                    f\"ROUNDUP({get_column_letter(dim_weight_col_idx)}{row}, 0), ROUNDUP({get_column_letter(total_weight_col_idx)}{row}, 0))\"\n",
        "                )\n",
        "\n",
        "            final_df = combined_df[difference_columns]\n",
        "\n",
        "    print(f\"Comparison Excel saved at {comparison_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory = \"Upload_Path\"\n",
        "    output_directory = \"Output_Path\"\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    process_files(directory, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnqT2PYnupSQ",
        "outputId": "353a05af-c592-46d2-c3ba-32490f48468f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741639578267,
          "user_tz": 420,
          "elapsed": 612646,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "OnqT2PYnupSQ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest sim_result_id for sim_id=2626: 3300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files:   3%|\u001b[34mâ–Ž         \u001b[0m| 1/32 [00:00<00:03,  9.37file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.11 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:   6%|\u001b[34mâ–‹         \u001b[0m| 2/32 [00:00<00:03,  8.97file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.11 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:   9%|\u001b[34mâ–‰         \u001b[0m| 3/32 [00:00<00:03,  9.24file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.10 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.06 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  16%|\u001b[34mâ–ˆâ–Œ        \u001b[0m| 5/32 [00:00<00:02, 12.11file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.06 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  22%|\u001b[34mâ–ˆâ–ˆâ–       \u001b[0m| 7/32 [00:00<00:02, 12.10file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.10 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.06 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.06 seconds to execute.\n",
            "Warning: No valid suffix in file Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output_cartons.\n",
            "Function 'load_and_process_file_in_chunks' took 0.03 seconds to execute.\n",
            "Warning: No valid suffix in file Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  31%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–      \u001b[0m| 10/32 [00:00<00:01, 15.31file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.05 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.16 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  38%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–Š      \u001b[0m| 12/32 [00:01<00:01, 10.32file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.16 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.18 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  44%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–     \u001b[0m| 14/32 [00:01<00:02,  8.23file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.17 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.17 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  50%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     \u001b[0m| 16/32 [00:01<00:02,  7.40file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.16 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  56%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    \u001b[0m| 18/32 [00:01<00:01,  9.06file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No valid suffix in file Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2543/gcs_sim_result_id=3201/results_data/raw/opc/opc.opc-0.output_cartons.\n",
            "Function 'load_and_process_file_in_chunks' took 0.05 seconds to execute.\n",
            "Warning: No valid suffix in file Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2543/gcs_sim_result_id=3201/results_data/raw/opc/opc.opc-0.output.\n",
            "Function 'load_and_process_file_in_chunks' took 0.05 seconds to execute.\n",
            "Function 'load_and_process_file_in_chunks' took 0.37 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  62%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   \u001b[0m| 20/32 [00:02<00:02,  5.91file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.23 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  66%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   \u001b[0m| 21/32 [00:02<00:02,  5.32file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.27 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  69%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   \u001b[0m| 22/32 [00:03<00:02,  4.91file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.26 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  72%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  \u001b[0m| 23/32 [00:03<00:01,  4.75file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.23 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  75%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  \u001b[0m| 24/32 [00:03<00:01,  4.11file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.34 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  78%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  \u001b[0m| 25/32 [00:03<00:01,  4.15file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.23 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  81%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– \u001b[0m| 26/32 [00:04<00:01,  4.18file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.23 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  84%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– \u001b[0m| 27/32 [00:04<00:01,  3.94file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.29 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  88%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š \u001b[0m| 28/32 [00:04<00:01,  3.73file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.30 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading files:  91%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ \u001b[0m| 29/32 [00:04<00:00,  3.88file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.23 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 32/32 [00:05<00:00,  6.10file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'load_and_process_file_in_chunks' took 0.30 seconds to execute.\n",
            "Warning: No valid suffix in file Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2590/gcs_sim_result_id=3255/results_data/raw/opc/opc.opc-0.output_cartons.\n",
            "Function 'load_and_process_file_in_chunks' took 0.02 seconds to execute.\n",
            "Warning: No valid suffix in file Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2590/gcs_sim_result_id=3255/results_data/raw/opc/opc.opc-0.output.\n",
            "Function 'load_and_process_file_in_chunks' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated output saved at: Output_Path/consolidated_output.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 618.43file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 710.76file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 583.69file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 624.37file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 622.56file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 490.12file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 595.49file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n",
            "Skipping sheet for file pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output_cartons due to invalid suffix.\n",
            "Skipping sheet for file pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/opc/opc.opc-0.output due to invalid suffix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 745.48file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 554.06file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 615.75file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 604.53file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 614.63file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 410.76file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n",
            "Skipping sheet for file pacsimulate_simulations_demo/gcs_sim_id=2543/gcs_sim_result_id=3201/results_data/raw/opc/opc.opc-0.output_cartons due to invalid suffix.\n",
            "Skipping sheet for file pacsimulate_simulations_demo/gcs_sim_id=2543/gcs_sim_result_id=3201/results_data/raw/opc/opc.opc-0.output due to invalid suffix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 685.96file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 526.57file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 540.06file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 597.31file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 522.57file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 447.55file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 468.85file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 516.21file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 619.02file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 593.13file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 509.19file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.02 seconds to execute.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Differences: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 6/6 [00:00<00:00, 596.22file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'compute_differences' took 0.01 seconds to execute.\n",
            "Skipping sheet for file pacsimulate_simulations_demo/gcs_sim_id=2590/gcs_sim_result_id=3255/results_data/raw/opc/opc.opc-0.output_cartons due to invalid suffix.\n",
            "Skipping sheet for file pacsimulate_simulations_demo/gcs_sim_id=2590/gcs_sim_result_id=3255/results_data/raw/opc/opc.opc-0.output due to invalid suffix.\n",
            "Comparison Excel saved at Output_Path/combined_output.xlsx\n",
            "Function 'process_files' took 611.84 seconds to execute.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Step ðŸ™ŒðŸ™ŒðŸ™Œ"
      ],
      "metadata": {
        "id": "7KjRed_0SfKo"
      },
      "id": "7KjRed_0SfKo"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2dc390d8-cc88-4edf-9360-d337b3d923aa",
      "metadata": {
        "id": "2dc390d8-cc88-4edf-9360-d337b3d923aa",
        "outputId": "f39d47bc-785e-475a-9ac7-89708362a31c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741639748055,
          "user_tz": 420,
          "elapsed": 165908,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest sim_result_id for sim_id=2626: 3300\n",
            "All plots have been successfully generated and saved.\n"
          ]
        }
      ],
      "source": [
        "#Step 2\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "sim_result_id_pattern = re.compile(\n",
        "    rf'gcs_sim_id={sim_id}/gcs_sim_result_id=(\\d+)/'\n",
        ")\n",
        "blobs = list(bucket.list_blobs(prefix=f\"pacsimulate_simulations_demo/gcs_sim_id={sim_id}/\"))\n",
        "sim_result_ids = []\n",
        "\n",
        "for blob in blobs:\n",
        "    match = sim_result_id_pattern.search(blob.name)\n",
        "    if match:\n",
        "        sim_result_ids.append(int(match.group(1)))\n",
        "\n",
        "if sim_result_ids:\n",
        "    largest_sim_result_id = max(sim_result_ids)\n",
        "    print(f\"Largest sim_result_id for sim_id={sim_id}: {largest_sim_result_id}\")\n",
        "else:\n",
        "    raise ValueError(f\"No results found for sim_id={sim_id}\")\n",
        "\n",
        "# Define the path to the Excel file\n",
        "excel_file = \"Output_Path/consolidated_output.xlsx\"\n",
        "output_dir = os.path.abspath(os.path.dirname(excel_file))\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Custom color palette\n",
        "custom_colors = {\n",
        "    \"gray\": \"#6a6a6b\",\n",
        "    \"blue\": \"#0400f9\",\n",
        "    \"seafoam\": \"#00ff80\",\n",
        "    \"bright_purple\": \"#8100fb\",\n",
        "    \"sky_blue\": \"#0A56B1\",\n",
        "    \"green\": \"#307f2f\",\n",
        "    \"brick\": \"#7f432f\",\n",
        "    \"red\": \"#ff0000\",\n",
        "    \"orange\": \"#ee6200\"\n",
        "}\n",
        "\n",
        "# Function to format y-axis\n",
        "format_thousands = lambda x, _: f'{int(x):,}'\n",
        "\n",
        "# Function to group carton types\n",
        "def group_carton_types(carton_name):\n",
        "    if isinstance(carton_name, float) or carton_name is None:\n",
        "        return \"Other\"  # Handle NaN cases\n",
        "\n",
        "    carton_name = str(carton_name)  # Convert to string\n",
        "    if \"Box\" in carton_name:\n",
        "        if \"Small\" in carton_name:\n",
        "            return \"Small Boxes\"\n",
        "        elif \"Medium\" in carton_name:\n",
        "            return \"Medium Boxes\"\n",
        "        elif \"Large\" in carton_name:\n",
        "            return \"Large Boxes\"\n",
        "    elif \"Mailer\" in carton_name:\n",
        "        return \"Mailers\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "\n",
        "# Read Excel sheets\n",
        "excel_data = pd.ExcelFile(excel_file)\n",
        "sheets = excel_data.sheet_names\n",
        "\n",
        "# Generate plots for each sheet\n",
        "for sheet in sheets:\n",
        "    df = pd.read_excel(excel_file, sheet_name=sheet)\n",
        "\n",
        "    # Group carton types\n",
        "    if 'name' in df.columns:\n",
        "        df['grouped_name'] = df['name'].apply(group_carton_types)\n",
        "\n",
        "    # Convert data types\n",
        "    df['surface_area'] = pd.to_numeric(df['surface_area'], errors='coerce')\n",
        "    df['Carton_volume'] = pd.to_numeric(df['Carton_volume'], errors='coerce')\n",
        "\n",
        "    # Filter missing values\n",
        "    df = df.dropna(subset=['total_weight', 'Billed_Weight', 'dim_weight', 'Price', 'base_cost', 'Order_volume', 'Carton_volume'])\n",
        "\n",
        "    # Summarize data\n",
        "    summary_data = df.groupby('source_flag').agg({\n",
        "        'Billed_Weight': 'sum',\n",
        "        'dim_weight': 'sum',\n",
        "        'total_weight': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "        # Combo Chart: Dim Weight vs Total Weight\n",
        "    if 'dim_weight' in df.columns and 'total_weight' in df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=summary_data, x='source_flag', y='dim_weight', color= custom_colors['gray'])\n",
        "        sns.lineplot(data=summary_data, x='source_flag', y=summary_data['total_weight'] / 2, color= custom_colors['seafoam'], marker='X', markersize=15, linewidth=3)\n",
        "        plt.title('Combo Chart: Dim Weight vs Total Weight')\n",
        "        plt.xlabel('Source Name')\n",
        "        plt.ylabel('Dim Weight')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"actual_vs_dim_weight.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Combo Chart: Billed Weight vs Total Weight\n",
        "    if 'Billed_Weight' in df.columns and 'total_weight' in df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=summary_data, x='source_flag', y='Billed_Weight', color= custom_colors['gray'])\n",
        "        sns.lineplot(data=summary_data, x='source_flag', y=summary_data['total_weight'] / 2, color= custom_colors['brick'], marker='X', markersize=15, linewidth=3)\n",
        "        plt.title('Combo Chart: Billed Weight vs Total Weight')\n",
        "        plt.xlabel('Source Name')\n",
        "        plt.ylabel('Billed Weight')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"actual_vs_billed_weight.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Surface Area Chart\n",
        "    if 'surface_area' in df.columns:\n",
        "        surface_data = df.groupby('source_flag')['surface_area'].sum().reset_index()\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(data=surface_data, x='source_flag', y='surface_area',  hue='source_flag', legend=False)\n",
        "        plt.title('Surface Area')\n",
        "        plt.xlabel('Source Name')\n",
        "        plt.ylabel('Total Surface Area')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"SA_agg_comp.png\"))\n",
        "        plt.close()\n",
        "\n",
        "     # Dimmed vs Count Chart\n",
        "    if 'Dimmed' in df.columns:\n",
        "        dimmed_data = df.groupby(['source_flag', 'Dimmed']).size().reset_index(name='count')\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=dimmed_data, x='source_flag', y='count', hue='Dimmed')\n",
        "        plt.title('Counts of Dimmed (Yes/No) by Source Flag')\n",
        "        plt.xlabel('Source Flag')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"dimmed_vs_count.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Dim Weight vs Billed Weight\n",
        "    if 'dim_weight' in df.columns and 'Billed_Weight' in df.columns:\n",
        "        weight_data = df.groupby('source_flag').agg({'dim_weight': 'sum', 'Billed_Weight': 'sum'}).reset_index()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=weight_data, x='source_flag', y='Billed_Weight', label='Billed Weight', color= custom_colors['gray'])\n",
        "        sns.lineplot(data=weight_data, x='source_flag', y='dim_weight', marker='X', linewidth=3, markersize=15, color= custom_colors['red'], label='Dim Weight')\n",
        "        plt.title('Sum of Dim Weight and Billed Weight by Source Flag')\n",
        "        plt.xlabel('Source Flag')\n",
        "        plt.ylabel('Total Weight')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"dim_weight_vs_billed_weight.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Orders Billed Over Actual\n",
        "    if 'Billed_over_Actual' in df.columns and 'orderId' in df.columns:\n",
        "        orders_data = df.groupby(['Billed_over_Actual', 'source_flag']).size().reset_index(name='order_count')\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.lineplot(data=orders_data, x='Billed_over_Actual', y='order_count', hue='source_flag', marker='X', linewidth=5, markersize=15)\n",
        "        plt.title('Combo Chart: Billed over Actual vs Count of Order ID by Source Flag')\n",
        "        plt.xlabel('Billed over Actual')\n",
        "        plt.ylabel('Count of Order ID')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"orders_billed_over_actual.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Price by Carton Type\n",
        "    if 'Price' in df.columns:\n",
        "        price_data = df.groupby(['name', 'source_flag']).agg({'Price': 'mean'}).reset_index()\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(data=price_data, x='name', y='Price', hue='source_flag')\n",
        "        plt.title('Average Price / Carton Type')\n",
        "        plt.xlabel('Carton Type')\n",
        "        plt.ylabel('Average Price')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"price_by_carton_type.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Volume Utilization by Carton Type\n",
        "    if 'volume_utilization' in df.columns:\n",
        "        volume_data = df.groupby(['name', 'source_flag']).agg({'volume_utilization': 'mean'}).reset_index()\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(data=volume_data, x='name', y='volume_utilization', hue='source_flag')\n",
        "        plt.legend(title='Source Flag', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "        plt.title('Average Volume Utilization / Carton Type')\n",
        "        plt.xlabel('Carton Type')\n",
        "        plt.ylabel('Average Volume Utilization')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"volume_utilization_by_carton_type.png\"))\n",
        "        plt.close()\n",
        "\n",
        "print(\"All plots have been successfully generated and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Step ðŸ™ŒðŸ™ŒðŸ™ŒðŸðŸðŸðŸ™ŒðŸ™ŒðŸ™Œ"
      ],
      "metadata": {
        "id": "aNqm6hFrShSg"
      },
      "id": "aNqm6hFrShSg"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "acbe7b0e-e089-4028-b61c-0d72f1162679",
      "metadata": {
        "id": "acbe7b0e-e089-4028-b61c-0d72f1162679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66dc7ff-5b90-45d3-fe20-638bfee163d4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741640086183,
          "user_tz": 420,
          "elapsed": 316234,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest sim_result_id for sim_id=2626: 3300\n",
            "Baseline file found: Upload_Path/pacsimulate_simulations_demo/gcs_sim_id=2626/gcs_sim_result_id=3300/results_data/raw/pacsimulate_2626.baseline.output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inserting Images into Excel: 100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 8/8 [00:00<00:00,  8.63image/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final file saved with visuals on a separate sheet: Carton_Output_run_2626.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from tqdm import tqdm\n",
        "from PIL import Image as PILImage\n",
        "from io import BytesIO\n",
        "\n",
        "sim_result_id_pattern = re.compile(\n",
        "    rf'gcs_sim_id={sim_id}/gcs_sim_result_id=(\\d+)/'\n",
        ")\n",
        "blobs = list(bucket.list_blobs(prefix=f\"pacsimulate_simulations_demo/gcs_sim_id={sim_id}/\"))\n",
        "sim_result_ids = []\n",
        "\n",
        "for blob in blobs:\n",
        "    match = sim_result_id_pattern.search(blob.name)\n",
        "    if match:\n",
        "        sim_result_ids.append(int(match.group(1)))\n",
        "\n",
        "if sim_result_ids:\n",
        "    largest_sim_result_id = max(sim_result_ids)\n",
        "    print(f\"Largest sim_result_id for sim_id={sim_id}: {largest_sim_result_id}\")\n",
        "else:\n",
        "    raise ValueError(f\"No results found for sim_id={sim_id}\")\n",
        "\n",
        "# Function to open the image for insertion into Excel\n",
        "def open_image(image_path, max_width=1200, max_height=1000):\n",
        "    try:\n",
        "        img = PILImage.open(image_path)  # Open the image using PIL\n",
        "        img_width, img_height = img.size\n",
        "        scale = min(max_width / img_width, max_height / img_height) # Calculate the resizing scale to maintain aspect ratio\n",
        "        new_width = int(img_width * scale) # Calculate new dimensions\n",
        "        new_height = int(img_height * scale)\n",
        "        img = img.resize((new_width, new_height), PILImage.Resampling.LANCZOS) # Resize the image\n",
        "        img_byte_arr = BytesIO() # Convert the resized image to a byte stream (in memory) for use in openpyxl\n",
        "        img.save(img_byte_arr, format='PNG')\n",
        "        img_byte_arr.seek(0)  # Reset pointer to the beginning of the byte stream\n",
        "        return Image(img_byte_arr) # Return the image for insertion into Excel\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {image_path} not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_files(directory, output_directory):\n",
        "    exclude_files = [\"preflight_baseline\", \"perfect\", \"all-candidates\"]\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # âœ… Search for files recursively & exclude unwanted ones\n",
        "    files = []\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            base_name = os.path.basename(filename)\n",
        "            if not any(excluded in base_name for excluded in exclude_files):\n",
        "                files.append(os.path.join(root, filename))\n",
        "    baseline_file = None\n",
        "    for filename in files:\n",
        "        if \"baseline\" in os.path.basename(filename).lower():\n",
        "            baseline_file = filename\n",
        "            break\n",
        "\n",
        "    if not baseline_file:\n",
        "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
        "\n",
        "    print(\"Baseline file found:\", baseline_file)\n",
        "\n",
        "    image_directory = 'Output_Path'\n",
        "    image_paths = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.endswith('.png')]\n",
        "\n",
        "    wb = load_workbook('Output_Path/combined_output.xlsx')  # Load the Excel file\n",
        "    ws_images = wb.create_sheet('Visuals')\n",
        "    current_row = 2  # Initialize row position\n",
        "    current_column = 2\n",
        "\n",
        "    with tqdm(total=len(image_paths), desc=\"Inserting Images into Excel\", unit=\"image\", colour=\"cyan\") as pbar:\n",
        "        for i, image_path in enumerate(image_paths):\n",
        "            img = open_image(image_path, max_width=1200, max_height=1000)\n",
        "\n",
        "            if img:\n",
        "                cell_position = f\"{chr(64 + current_column)}{current_row}\"\n",
        "                ws_images.add_image(img, cell_position)\n",
        "                # Adjust row and column for the next image\n",
        "                current_column += 50  # Move to the next column\n",
        "                if current_column > 1:  # Adjust number of columns per row\n",
        "                    current_column = 2  # Reset to column D\n",
        "                    current_row += 50  # Move to the next row\n",
        "            else:\n",
        "                print(f\"Image {image_path} could not be opened.\")  # Debugging log\n",
        "            pbar.update(1)\n",
        "\n",
        "    final_filename = f\"Carton_Output_run_{sim_id}.xlsx\" # Define the new output file name\n",
        "    final_path = os.path.join(output_directory, final_filename)\n",
        "\n",
        "    wb.save(final_path)\n",
        "    print(f\"Final file saved with visuals on a separate sheet: {final_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory = \"Upload_Path\"\n",
        "    output_directory = \"Output_Path\"\n",
        "    process_files(directory, output_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find your download files located in the file folder to the left under **Output_Path** âœ” ðŸ‘€\n",
        "\n",
        "*it can take up to 15 minutes for a file to appear, reload the page or press the refresh button atop the file tray*"
      ],
      "metadata": {
        "id": "IqSxP7V7S7Ly"
      },
      "id": "IqSxP7V7S7Ly"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WH3g-pULrK2A"
      },
      "id": "WH3g-pULrK2A",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "name": "Transmutr.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}