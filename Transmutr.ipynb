{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e7aea-3d51-4990-babc-2edb88a01f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f339b6-f458-4967-8493-5c1844840e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.styles import NamedStyle\n",
    "\n",
    "# Decorator to measure execution time\n",
    "def time_complexity(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' took {end_time - start_time:.2f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "column_mapping = {\n",
    "    \"fulfillment_id\": \"orderId\",\n",
    "    \"ref_id\": \"refId\",\n",
    "    \"index\": \"index\",\n",
    "    \"name\": \"name\",\n",
    "    \"dimensions\": \"dimensions\",\n",
    "    \"cost\": \"Price\",\n",
    "    \"base_cost\": \"base_cost\",\n",
    "    \"total_volume\": \"Carton_volume\",\n",
    "    \"net_volume\": \"Order_volume\",\n",
    "    \"volume_utilization\": \"volume_utilization\",\n",
    "    \"surface_area\": \"surface_area\",\n",
    "    \"total_weight\": \"total_weight\",\n",
    "    \"net_weight\": \"net_weight\",\n",
    "    \"tare_weight\": \"tare_weight\",\n",
    "    \"weight_utilization\": \"weight_utilization\",\n",
    "    \"item_count\": \"item_count\",\n",
    "    \"dim_weight\": \"dim_weight\",\n",
    "}\n",
    "\n",
    "# Columns for consolidated output\n",
    "consolidated_columns = [\n",
    "    \"orderId\", \"refId\", \"index\", \"name\", \"dimensions\", \"Price\", \"base_cost\",\n",
    "    \"Carton_volume\", \"Order_volume\", \"volume_utilization\", \"surface_area\",\n",
    "    \"total_weight\", \"net_weight\", \"tare_weight\", \"weight_utilization\",\n",
    "    \"dim_weight\", \"item_count\", \"source_flag\"\n",
    "]\n",
    "\n",
    "# Combined sheet column order\n",
    "combined_columns = [\n",
    "    \"orderId_baseline\", \"orderId\", \"refId_baseline\", \"refId\",\n",
    "    \"index_baseline\", \"index\", \"name_baseline\", \"name\",\n",
    "    \"dimensions_baseline\", \"dimensions\", \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
    "    \"base_cost_baseline\", \"base_cost\", \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\",\n",
    "    \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\", \"volume_utilization_baseline\", \"volume_utilization\", \n",
    "    \"volume_utilization_Diff\", \"surface_area_baseline\", \"surface_area\", \"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\",\n",
    "    \"net_weight_baseline\", \"net_weight\", \"tare_weight_baseline\", \"tare_weight\",\n",
    "    \"weight_utilization_baseline\", \"weight_utilization\", \"dim_weight_baseline\", \"dim_weight\",\n",
    "    \"item_count_baseline\", \"item_count\", \"Item_Diff\"\n",
    "]\n",
    "\n",
    " # Columns to drop to save memory\n",
    "columns_to_drop = [\"item_summary\"]  \n",
    "\n",
    "\n",
    "# Refined suffix extraction:\n",
    "def refine_suffix(filename):\n",
    "    \"\"\"Extracts suffix between 'filepath_####.' and next period/end\"\"\"\n",
    "    match = re.search(r\"filepath_(\\d+)\\.(.*?)(?:$|\\.)\", filename)\n",
    "    test = re.search(r\"transmutr\",filename)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    else:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "@time_complexity\n",
    "# Loads a file in chunks, processes it, and returns a DataFrame\n",
    "def load_and_process_file_in_chunks(file_path, chunk_size=100000):\n",
    "    try:\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, delimiter='|', low_memory=False, memory_map=True,\n",
    "            on_bad_lines='skip', chunksize=chunk_size\n",
    "        ):\n",
    "            # Drop unnecessary columns\n",
    "            if columns_to_drop:\n",
    "                chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "            # Rename columns based on mapping\n",
    "            chunk = chunk.rename(columns={col: column_mapping[col] for col in column_mapping if col in chunk.columns})\n",
    "\n",
    "            # Extract the source_flag using the filename suffix\n",
    "            suffix = refine_suffix(file_path)\n",
    "            if suffix:\n",
    "                chunk[\"source_flag\"] = suffix\n",
    "            else:\n",
    "                print(f\"Warning: No valid suffix in file {file_path}.\")\n",
    "\n",
    "            # Reindex to ensure all required columns are present\n",
    "            chunk = chunk.reindex(columns=consolidated_columns, fill_value=None)\n",
    "\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_consolidated_fields(df):\n",
    "    df['dimensions'] = df['dimensions'].astype(str).fillna('')\n",
    "    # Calculate Dimmed\n",
    "    df['Dimmed'] = np.where(df['dim_weight'] > df['total_weight'], 'Yes', 'No')\n",
    "\n",
    "    # Calculate Billed Weight\n",
    "    df['Billed_Weight'] = np.where(df['dim_weight'] > df['total_weight'], np.ceil(df['dim_weight']), np.ceil(df['total_weight'])).astype(int)\n",
    "\n",
    "    # Billed Over Actual\n",
    "    df['total_weight'] = np.ceil(df['total_weight'])\n",
    "\n",
    "    # Then calculate the 'Billed_over_Actual' column\n",
    "    df['Billed_over_Actual'] = np.where(df['Billed_Weight'] - df['total_weight'] > 0, df['Billed_Weight'] - df['total_weight'], 0)\n",
    "\n",
    "    # Split dimensions into L, W, H\n",
    "    dimensions_split = df['dimensions'].str.split(',', expand=True)\n",
    "\n",
    "    # Validate that the split resulted in exactly three parts\n",
    "    if dimensions_split.shape[1] != 3:\n",
    "        print(\"Warning: 'dimensions' column does not split into exactly three parts (L,W,H). Filling with NaN.\")\n",
    "        dimensions_split = dimensions_split.reindex(columns=[0,1,2], fill_value=np.nan)\n",
    "\n",
    "    # Assign to new columns\n",
    "    df['L'] = pd.to_numeric(dimensions_split[0].str.strip(), errors='coerce').fillna(0)\n",
    "    df['W'] = pd.to_numeric(dimensions_split[1].str.strip(), errors='coerce').fillna(0)\n",
    "    df['H'] = pd.to_numeric(dimensions_split[2].str.strip(), errors='coerce').fillna(0)\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "@time_complexity\n",
    "def compute_differences(df):\n",
    "    try:\n",
    "        # Define the column pairs for differences\n",
    "        column_pairs = [\n",
    "            (\"Price_baseline\", \"Price\", \"Price_Diff\"),\n",
    "            (\"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\"),\n",
    "            (\"item_count_baseline\", \"item_count\", \"Item_Diff\"),\n",
    "            (\"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\"),\n",
    "            (\"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\"),\n",
    "            (\"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\"),\n",
    "        ]\n",
    "\n",
    "        for col_baseline, col, col_diff in tqdm(\n",
    "            column_pairs, desc=\"Computing Differences\", unit=\"file\", colour=\"green\"\n",
    "        ):\n",
    "            if col_baseline in df.columns and col in df.columns:\n",
    "                df[col_baseline] = pd.to_numeric(df[col_baseline], errors=\"coerce\").fillna(0)\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "                df[col_diff] = df[col] - df[col_baseline]\n",
    "            else:\n",
    "                print(f\"Skipping difference calculation for {col_diff}: Missing {col_baseline} or {col}\")\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_differences: {e}\")\n",
    "        return df\n",
    "\n",
    "def apply_dynamic_column_formats(sheet):\n",
    "    # Define the number style with 2 decimal places for numeric columns\n",
    "    number_style = NamedStyle(name=\"number\")\n",
    "    number_style.number_format = '0.00'  # Number format for 2 decimal places\n",
    "\n",
    "    # Define the percentage style for utilization columns\n",
    "    percentage_style = NamedStyle(name=\"percentage\")\n",
    "    percentage_style.number_format = '0.00%'  # Percentage format for utilization columns\n",
    "\n",
    "    # List of columns to apply number formatting (Price, Carton Volume, etc.)\n",
    "    numeric_columns = [\n",
    "        \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
    "        \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\",\n",
    "        \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\",\n",
    "        \"surface_area_baseline\", \"surface_area\",\n",
    "        \"dim_weight_baseline\", \"dim_weight\"\n",
    "    ]\n",
    "\n",
    "    # List of columns for utilization percentage formatting\n",
    "    utilization_columns = [\n",
    "        \"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\",\n",
    "        \"weight_utilization_baseline\", \"weight_utilization\"\n",
    "    ]\n",
    "\n",
    "    # Loop through columns and apply styles based on column name\n",
    "    for col in sheet.columns:\n",
    "        column_letter = col[0].column_letter\n",
    "        column_name = str(col[0].value).strip() if col[0].value else \"\"\n",
    "\n",
    "        # Apply number format to numeric columns\n",
    "        if column_name in numeric_columns:\n",
    "            for cell in col:\n",
    "                cell.number_format = '0.00'  # Apply number format with 2 decimal places\n",
    "\n",
    "        # Apply percentage format to utilization columns\n",
    "        elif column_name in utilization_columns:\n",
    "            for cell in col:\n",
    "                cell.number_format = '0.00%'  # Apply percentage format\n",
    "\n",
    "@time_complexity\n",
    "def process_files(directory, output_directory):\n",
    "    data_frames = {}\n",
    "    consolidated_data = []\n",
    "    files = []\n",
    "    '''for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            base_name = os.path.basename(filename)\n",
    "            if not any(excluded in base_name for excluded in exclude_files):  # âœ… Exclude unwanted files\n",
    "                files.append(os.path.relpath(os.path.join(root, filename), directory))  # âœ… Fix double \"Upload_Path\"'''\n",
    "\n",
    "    if not files:\n",
    "        print(\"No matching files found for processing.\")\n",
    "        return\n",
    "\n",
    "    # âœ… Ensure a valid baseline file exists\n",
    "    baseline_files = [f for f in files if \"baseline\" in os.path.basename(f).lower()]\n",
    "    if not baseline_files:\n",
    "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
    "    baseline_file = baseline_files[0]  # Take the first matching file\n",
    "\n",
    "    print(f\"Processing {len(files)} files (excluding preflight, perfect, all-candidates).\")\n",
    "\n",
    "    # Process files\n",
    "    with tqdm(total=len(files), desc=\"Loading files\", unit=\"file\", colour=\"blue\") as pbar:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = load_and_process_file_in_chunks(file_path, chunk_size=100000)\n",
    "            if not df.empty:\n",
    "                price_columns = (\"Price_baseline\", \"Price\", \"Price_Diff\")\n",
    "                for col in price_columns:\n",
    "                    if col in df.columns:\n",
    "                        # Convert column to numeric , then multiply by 100\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce') / 100\n",
    "                data_frames[filename] = df\n",
    "                consolidated_data.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: No valid data in file {filename}\")\n",
    "            pbar.update(n=1)\n",
    "\n",
    "\n",
    "    # Consolidate all data into a single DataFrame\n",
    "    if consolidated_data:\n",
    "        consolidated_output = pd.concat(consolidated_data, ignore_index=True)\n",
    "        try:\n",
    "            # Perform any calculations\n",
    "            consolidated_output = calculate_consolidated_fields(consolidated_output)\n",
    "\n",
    "            # Save the consolidated output\n",
    "            consolidated_output_path = os.path.join(output_directory, \"consolidated_output.xlsx\")\n",
    "            with pd.ExcelWriter(consolidated_output_path, engine='openpyxl') as writer:\n",
    "                consolidated_output.to_excel(writer, sheet_name=\"Consolidated_Output\", index=False)\n",
    "\n",
    "                workbook = writer.book\n",
    "                sheet = writer.sheets[\"Consolidated_Output\"]\n",
    "\n",
    "\n",
    "                # Header formating\n",
    "                for cell in sheet[1]:\n",
    "                    cell.font = Font(bold=False)\n",
    "                # column formatting\n",
    "                for col in sheet.columns:\n",
    "                    column_letter = col[0].column_letter\n",
    "                    # Get the maximum length of the column header and its values\n",
    "                    header_length = len(str(col[0].value)) if col[0].value else 0\n",
    "                    # Calculate the column width\n",
    "                    column_width = max(header_length + 0.5, 13)\n",
    "                    # Set the column width\n",
    "                    sheet.column_dimensions[column_letter].width = column_width\n",
    "\n",
    "            print(f\"Consolidated output saved at: {consolidated_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during consolidated calculations or saving: {e}\")\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")\n",
    "    # Define the new output file name\n",
    "    comparison_filename = \"combined_output.xlsx\"\n",
    "    comparison_path = os.path.join(output_directory, comparison_filename)\n",
    "\n",
    "    # ðŸš€ Before attempting to pop, ensure baseline_file exists in data_frames\n",
    "    if baseline_file not in data_frames:\n",
    "        print(f\"âŒ ERROR: Baseline file '{baseline_file}' was NOT found in processed data!\")\n",
    "        print(f\"ðŸ”¹ Available keys in data_frames: {list(data_frames.keys())}\")\n",
    "        print(f\"ðŸ” Check if '{baseline_file}' exists in matching_files: {baseline_file in matching_files}\")\n",
    "        return  # ðŸš¨ Stop execution safely\n",
    "\n",
    "    # ðŸš€ Now, pop the baseline file safely\n",
    "    baseline_df = data_frames.pop(baseline_file)\n",
    "\n",
    "    # ðŸš€ Debugging Print to Confirm It's Loaded\n",
    "    print(f\"âœ… Successfully loaded baseline file: {baseline_file}\")\n",
    "\n",
    "\n",
    "    # Save comparison outputs to Excel\n",
    "    #baseline_df = data_frames.pop(baseline_file)\n",
    "    with pd.ExcelWriter(comparison_path, engine='openpyxl') as writer:\n",
    "        for key, df in data_frames.items():\n",
    "            sheet_name = refine_suffix(key)  # Using the refined filename as sheet name\n",
    "            if not sheet_name:\n",
    "                print(f\"Skipping sheet for file {key} due to invalid suffix.\")\n",
    "                continue  # Skip empty suffix\n",
    "\n",
    "            # Align comparison DataFrame columns with combined_columns\n",
    "            comparison_df = df.reindex(columns=[col.replace(\"_baseline\", \"\") for col in combined_columns if \"_baseline\" not in col])\n",
    "\n",
    "            # Combine baseline and comparison data\n",
    "            combined_df = pd.concat(\n",
    "                [baseline_df.add_suffix(\"_baseline\"), comparison_df],\n",
    "                axis=1\n",
    "            ).reindex(columns=combined_columns)\n",
    "\n",
    "            # Compute differences\n",
    "            combined_df = compute_differences(combined_df)\n",
    "\n",
    "            # Keep the difference columns\n",
    "            difference_columns = [\n",
    "                \"Price_Diff\", \"Order_volume_Diff\", \"Item_Diff\",\n",
    "                \"Carton_volume_Diff\", \"volume_utilization_Diff\", \"total_weight_Diff\"\n",
    "            ]\n",
    "\n",
    "            if combined_df.empty:\n",
    "                print(f\"Warning: Sheet {sheet_name} has no data. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Write the combined DataFrame to the corresponding sheet\n",
    "            combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            # Apply formulas for Dimmed and Billed Weight\n",
    "            workbook = writer.book\n",
    "            sheet = writer.sheets[sheet_name]\n",
    "\n",
    "            # Apply dynamic formatting\n",
    "            apply_dynamic_column_formats(sheet)\n",
    "\n",
    "            # Header formating\n",
    "            for cell in sheet[1]:\n",
    "                cell.font = Font(bold=False)\n",
    "\n",
    "            # Column width adjustments\n",
    "            for col in sheet.columns:\n",
    "                column_letter = col[0].column_letter\n",
    "                header_length = len(str(col[0].value)) if col[0].value else 0\n",
    "                column_width = max(header_length + 0.5, 13)\n",
    "                sheet.column_dimensions[column_letter].width = column_width\n",
    "\n",
    "            headers = [str(cell.value).strip() for cell in sheet[1]]\n",
    "            dim_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"dim_weight_\")), None)\n",
    "            total_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"total_weight_\")), None)\n",
    "\n",
    "            if dim_weight_col_idx is None or total_weight_col_idx is None:\n",
    "                print(f\"Warning: Required columns for Dimmed and Billed Weight formulas not found in '{sheet_name}'. Headers: {headers}\")\n",
    "                continue\n",
    "\n",
    "            last_column = len(headers)\n",
    "            dimmed_col_letter = get_column_letter(last_column + 1)\n",
    "            billed_weight_col_letter = get_column_letter(last_column + 2)\n",
    "\n",
    "            sheet[f\"{dimmed_col_letter}1\"] = \"Dimmed\"\n",
    "            sheet[f\"{billed_weight_col_letter}1\"] = \"Billed_Weight\"\n",
    "\n",
    "            for row in range(2, sheet.max_row + 1):\n",
    "                sheet[f\"{dimmed_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \\\"Yes\\\", \\\"No\\\")\"\n",
    "                )\n",
    "                sheet[f\"{billed_weight_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \"\n",
    "                    f\"ROUNDUP({get_column_letter(dim_weight_col_idx)}{row}, 0), ROUNDUP({get_column_letter(total_weight_col_idx)}{row}, 0))\"\n",
    "                )\n",
    "\n",
    "            final_df = combined_df[difference_columns]\n",
    "\n",
    "    print(f\"Comparison Excel saved at {comparison_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"Upload_Path\"\n",
    "    output_directory = \"Output_Path\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    process_files(directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84476c09-e49b-49b2-ae2c-d71efe963256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 1\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.styles import NamedStyle\n",
    "\n",
    "# Decorator to measure execution time\n",
    "def time_complexity(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' took {end_time - start_time:.2f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Column rename mapping\n",
    "column_mapping = {\n",
    "    \"fulfillment_id\": \"orderId\",\n",
    "    \"ref_id\": \"refId\",\n",
    "    \"index\": \"index\",\n",
    "    \"name\": \"name\",\n",
    "    \"dimensions\": \"dimensions\",\n",
    "    \"cost\": \"Price\",\n",
    "    \"base_cost\": \"base_cost\",\n",
    "    \"total_volume\": \"Carton_volume\",\n",
    "    \"net_volume\": \"Order_volume\",\n",
    "    \"volume_utilization\": \"volume_utilization\",\n",
    "    \"surface_area\": \"surface_area\",\n",
    "    \"total_weight\": \"total_weight\",\n",
    "    \"net_weight\": \"net_weight\",\n",
    "    \"tare_weight\": \"tare_weight\",\n",
    "    \"weight_utilization\": \"weight_utilization\",\n",
    "    \"item_count\": \"item_count\",\n",
    "    \"dim_weight\": \"dim_weight\",\n",
    "}\n",
    "\n",
    "# Columns for consolidated output\n",
    "consolidated_columns = [\n",
    "    \"orderId\", \"refId\", \"index\", \"name\", \"dimensions\", \"Price\", \"base_cost\",\n",
    "    \"Carton_volume\", \"Order_volume\", \"volume_utilization\", \"surface_area\",\n",
    "    \"total_weight\", \"net_weight\", \"tare_weight\", \"weight_utilization\",\n",
    "    \"dim_weight\", \"item_count\", \"source_flag\"\n",
    "]\n",
    "\n",
    "# Combined sheet column order\n",
    "combined_columns = [\n",
    "    \"orderId_baseline\", \"orderId\", \"refId_baseline\", \"refId\",\n",
    "    \"index_baseline\", \"index\", \"name_baseline\", \"name\",\n",
    "    \"dimensions_baseline\", \"dimensions\", \"Price_baseline\", \"Price\", \"Price_Diff\",\n",
    "    \"base_cost_baseline\", \"base_cost\", \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\",\n",
    "    \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\", \"volume_utilization_baseline\", \"volume_utilization\", \n",
    "    \"volume_utilization_Diff\", \"surface_area_baseline\", \"surface_area\", \"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\",\n",
    "    \"net_weight_baseline\", \"net_weight\", \"tare_weight_baseline\", \"tare_weight\",\n",
    "    \"weight_utilization_baseline\", \"weight_utilization\", \"dim_weight_baseline\", \"dim_weight\",\n",
    "    \"item_count_baseline\", \"item_count\", \"Item_Diff\"\n",
    "]\n",
    "\n",
    " # Columns to drop to save memory\n",
    "columns_to_drop = [\"item_summary\"]  \n",
    "\n",
    "# Refined suffix extraction:\n",
    "def refine_suffix(filename):\n",
    "    \"\"\"Extracts suffix between '_####.' and next period/end\"\"\"\n",
    "    match = re.search(r\"filepath_(\\d+)\\.(.*?)(?:$|\\.)\", filename)\n",
    "    test = re.search(r\"transmutr\",filename)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    else:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "@time_complexity\n",
    "# Loads a file in chunks, processes it, and returns a DataFrame\n",
    "def load_and_process_file_in_chunks(file_path, chunk_size=100000):\n",
    "    try:\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, delimiter='|', low_memory=False, memory_map=True,\n",
    "            on_bad_lines='skip', chunksize=chunk_size\n",
    "        ):\n",
    "            # Drop unnecessary columns\n",
    "            if columns_to_drop:\n",
    "                chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "            # Rename columns based on mapping\n",
    "            chunk = chunk.rename(columns={col: column_mapping[col] for col in column_mapping if col in chunk.columns})\n",
    "\n",
    "            # Extract the source_flag using the filename suffix\n",
    "            suffix = refine_suffix(file_path)\n",
    "            if suffix:\n",
    "                chunk[\"source_flag\"] = suffix\n",
    "            else:\n",
    "                print(f\"Warning: No valid suffix in file {file_path}.\")\n",
    "\n",
    "            # Reindex to ensure all required columns are present\n",
    "            chunk = chunk.reindex(columns=consolidated_columns, fill_value=None)\n",
    "\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_consolidated_fields(df):    \n",
    "    # Calculate Dimmed\n",
    "    df['Dimmed'] = np.where(df['dim_weight'] > df['total_weight'], 'Yes', 'No')\n",
    "\n",
    "    # Calculate Billed Weight\n",
    "    df['Billed_Weight'] = np.where(df['dim_weight'] > df['total_weight'], np.ceil(df['dim_weight']), np.ceil(df['total_weight'])).astype(int)\n",
    "    \n",
    "    # Billed Over Actual\n",
    "    df['total_weight'] = np.ceil(df['total_weight'])\n",
    "\n",
    "    # Then calculate the 'Billed_over_Actual' column\n",
    "    df['Billed_over_Actual'] = np.where(df['Billed_Weight'] - df['total_weight'] > 0, df['Billed_Weight'] - df['total_weight'], 0)\n",
    "\n",
    "    # Split dimensions into L, W, H\n",
    "    dimensions_split = df['dimensions'].str.split(',', expand=True)\n",
    "    \n",
    "    # Validate that the split resulted in exactly three parts\n",
    "    #if dimensions_split.shape[1] != 3:\n",
    "    #    print(\"Warning: 'dimensions' column does not split into exactly three parts (L,W,H). Filling with NaN.\")\n",
    "    #    dimensions_split = dimensions_split.reindex(columns=[0,1,2], fill_value=np.nan)\n",
    "\n",
    "    # Assign to new columns\n",
    "    df['L'] = pd.to_numeric(dimensions_split[0].str.strip(), errors='coerce')\n",
    "    df['W'] = pd.to_numeric(dimensions_split[1].str.strip(), errors='coerce')\n",
    "    df['H'] = pd.to_numeric(dimensions_split[2].str.strip(), errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "@time_complexity\n",
    "def compute_differences(df):\n",
    "    try:\n",
    "        # Define the column pairs for differences\n",
    "        column_pairs = [\n",
    "            (\"Price_baseline\", \"Price\", \"Price_Diff\"),\n",
    "            (\"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\"),\n",
    "            (\"item_count_baseline\", \"item_count\", \"Item_Diff\"),\n",
    "            (\"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\"),\n",
    "            (\"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\"),\n",
    "            (\"total_weight_baseline\", \"total_weight\", \"total_weight_Diff\"),\n",
    "        ]\n",
    "\n",
    "        for col_baseline, col, col_diff in tqdm(\n",
    "            column_pairs, desc=\"Computing Differences\", unit=\"file\", colour=\"green\"\n",
    "        ):\n",
    "            if col_baseline in df.columns and col in df.columns:\n",
    "                df[col_baseline] = pd.to_numeric(df[col_baseline], errors=\"coerce\").fillna(0)\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "                df[col_diff] = df[col] - df[col_baseline]\n",
    "            else:\n",
    "                print(f\"Skipping difference calculation for {col_diff}: Missing {col_baseline} or {col}\")\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_differences: {e}\")\n",
    "        return df\n",
    "\n",
    "def apply_dynamic_column_formats(sheet):\n",
    "    # Define the number style with 2 decimal places for numeric columns\n",
    "    number_style = NamedStyle(name=\"number\")\n",
    "    number_style.number_format = '0.00'  # Number format for 2 decimal places\n",
    "    \n",
    "    # Define the percentage style for utilization columns\n",
    "    percentage_style = NamedStyle(name=\"percentage\")\n",
    "    percentage_style.number_format = '0.00%'  # Percentage format for utilization columns\n",
    "\n",
    "    # List of columns to apply number formatting (Price, Carton Volume, etc.)\n",
    "    numeric_columns = [\n",
    "        \"Price_baseline\", \"Price\", \"Price_Diff\", \n",
    "        \"Carton_volume_baseline\", \"Carton_volume\", \"Carton_volume_Diff\", \n",
    "        \"Order_volume_baseline\", \"Order_volume\", \"Order_volume_Diff\", \n",
    "        \"surface_area_baseline\", \"surface_area\", \n",
    "        \"dim_weight_baseline\", \"dim_weight\"\n",
    "    ]\n",
    "    \n",
    "    # List of columns for utilization percentage formatting\n",
    "    utilization_columns = [\n",
    "        \"volume_utilization_baseline\", \"volume_utilization\", \"volume_utilization_Diff\", \n",
    "        \"weight_utilization_baseline\", \"weight_utilization\"\n",
    "    ]\n",
    "\n",
    "    # Loop through columns and apply styles based on column name\n",
    "    for col in sheet.columns:\n",
    "        column_letter = col[0].column_letter\n",
    "        column_name = str(col[0].value).strip() if col[0].value else \"\"\n",
    "\n",
    "        # Apply number format to numeric columns\n",
    "        if column_name in numeric_columns:\n",
    "            for cell in col:\n",
    "                cell.number_format = '0.00'  # Apply number format with 2 decimal places\n",
    "        \n",
    "        # Apply percentage format to utilization columns\n",
    "        elif column_name in utilization_columns:\n",
    "            for cell in col:\n",
    "                cell.number_format = '0.00%'  # Apply percentage format\n",
    "\n",
    "@time_complexity\n",
    "def process_files(directory, output_directory):\n",
    "    files = os.listdir(directory)\n",
    "    data_frames = {}\n",
    "    consolidated_data = []\n",
    "    baseline_file = None\n",
    "\n",
    "    # Identify the baseline file\n",
    "    for filename in files:\n",
    "        if \"baseline\" in filename.lower():\n",
    "            baseline_file = filename\n",
    "            break\n",
    "\n",
    "    if not baseline_file:\n",
    "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
    "\n",
    "    # Process files\n",
    "    with tqdm(total=len(files), desc=\"Loading files\", unit=\"file\", colour=\"blue\") as pbar:\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = load_and_process_file_in_chunks(file_path, chunk_size=100000)\n",
    "            if not df.empty:\n",
    "                price_columns = (\"Price_baseline\", \"Price\", \"Price_Diff\")\n",
    "                for col in price_columns:\n",
    "                    if col in df.columns:\n",
    "                        # Convert column to numeric , then multiply by 100\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce') / 100\n",
    "                data_frames[filename] = df\n",
    "                consolidated_data.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: No valid data in file {filename}\")\n",
    "            pbar.update(n=1)\n",
    "            \n",
    "\n",
    "    # Consolidate all data into a single DataFrame\n",
    "    if consolidated_data:\n",
    "        consolidated_output = pd.concat(consolidated_data, ignore_index=True)\n",
    "        try:\n",
    "            # Perform any calculations \n",
    "            consolidated_output = calculate_consolidated_fields(consolidated_output)\n",
    "            \n",
    "            # Save the consolidated output\n",
    "            consolidated_output_path = os.path.join(output_directory, \"consolidated_output.xlsx\")\n",
    "            with pd.ExcelWriter(consolidated_output_path, engine='openpyxl') as writer:\n",
    "                consolidated_output.to_excel(writer, sheet_name=\"Consolidated_Output\", index=False)\n",
    "\n",
    "                workbook = writer.book\n",
    "                sheet = writer.sheets[\"Consolidated_Output\"]\n",
    "                \n",
    "                \n",
    "                # Header formating \n",
    "                for cell in sheet[1]:\n",
    "                    cell.font = Font(bold=False)\n",
    "                # column formatting\n",
    "                for col in sheet.columns:\n",
    "                    column_letter = col[0].column_letter\n",
    "                    # Get the maximum length of the column header and its values\n",
    "                    header_length = len(str(col[0].value)) if col[0].value else 0\n",
    "                    # Calculate the column width\n",
    "                    column_width = max(header_length + 0.5, 13)\n",
    "                    # Set the column width\n",
    "                    sheet.column_dimensions[column_letter].width = column_width\n",
    "\n",
    "            print(f\"Consolidated output saved at: {consolidated_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during consolidated calculations or saving: {e}\")\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")\n",
    "\n",
    "    # Define the new output file name\n",
    "    comparison_filename = \"combined_output.xlsx\"\n",
    "    comparison_path = os.path.join(output_directory, comparison_filename)\n",
    "    \n",
    "    # Save comparison outputs to Excel\n",
    "    baseline_df = data_frames.pop(baseline_file)\n",
    "    with pd.ExcelWriter(comparison_path, engine='openpyxl') as writer:\n",
    "        for key, df in data_frames.items():\n",
    "            sheet_name = refine_suffix(key)  # Using the refined filename as sheet name\n",
    "            if not sheet_name:\n",
    "                print(f\"Skipping sheet for file {key} due to invalid suffix.\")\n",
    "                continue  # Skip empty suffix\n",
    "            \n",
    "            # Align comparison DataFrame columns with combined_columns\n",
    "            comparison_df = df.reindex(columns=[col.replace(\"_baseline\", \"\") for col in combined_columns if \"_baseline\" not in col])\n",
    "\n",
    "            # Combine baseline and comparison data\n",
    "            combined_df = pd.concat(\n",
    "                [baseline_df.add_suffix(\"_baseline\"), comparison_df],\n",
    "                axis=1\n",
    "            ).reindex(columns=combined_columns)\n",
    "\n",
    "            # Compute differences\n",
    "            combined_df = compute_differences(combined_df)\n",
    "\n",
    "            # Keep the difference columns\n",
    "            difference_columns = [\n",
    "                \"Price_Diff\", \"Order_volume_Diff\", \"Item_Diff\", \n",
    "                \"Carton_volume_Diff\", \"volume_utilization_Diff\", \"total_weight_Diff\"\n",
    "            ]\n",
    "\n",
    "            if combined_df.empty:\n",
    "                print(f\"Warning: Sheet {sheet_name} has no data. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Write the combined DataFrame to the corresponding sheet\n",
    "            combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            # Apply formulas for Dimmed and Billed Weight\n",
    "            workbook = writer.book\n",
    "            sheet = writer.sheets[sheet_name]\n",
    "\n",
    "            # Apply dynamic formatting\n",
    "            apply_dynamic_column_formats(sheet)\n",
    "\n",
    "            # Header formating \n",
    "            for cell in sheet[1]:\n",
    "                cell.font = Font(bold=False)\n",
    "\n",
    "            # Column width adjustments\n",
    "            for col in sheet.columns:\n",
    "                column_letter = col[0].column_letter\n",
    "                header_length = len(str(col[0].value)) if col[0].value else 0\n",
    "                column_width = max(header_length + 0.5, 13)\n",
    "                sheet.column_dimensions[column_letter].width = column_width\n",
    "\n",
    "            headers = [str(cell.value).strip() for cell in sheet[1]]\n",
    "            dim_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"dim_weight_\")), None)\n",
    "            total_weight_col_idx = next((i + 1 for i, h in enumerate(headers) if h.startswith(\"total_weight_\")), None)\n",
    "\n",
    "            if dim_weight_col_idx is None or total_weight_col_idx is None:\n",
    "                print(f\"Warning: Required columns for Dimmed and Billed Weight formulas not found in '{sheet_name}'. Headers: {headers}\")\n",
    "                continue\n",
    "\n",
    "            last_column = len(headers)\n",
    "            dimmed_col_letter = get_column_letter(last_column + 1)\n",
    "            billed_weight_col_letter = get_column_letter(last_column + 2)\n",
    "\n",
    "            sheet[f\"{dimmed_col_letter}1\"] = \"Dimmed\"\n",
    "            sheet[f\"{billed_weight_col_letter}1\"] = \"Billed_Weight\"\n",
    "\n",
    "            for row in range(2, sheet.max_row + 1):\n",
    "                sheet[f\"{dimmed_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \\\"Yes\\\", \\\"No\\\")\"\n",
    "                )\n",
    "                sheet[f\"{billed_weight_col_letter}{row}\"] = (\n",
    "                    f\"=IF({get_column_letter(dim_weight_col_idx)}{row} > {get_column_letter(total_weight_col_idx)}{row}, \"\n",
    "                    f\"ROUNDUP({get_column_letter(dim_weight_col_idx)}{row}, 0), ROUNDUP({get_column_letter(total_weight_col_idx)}{row}, 0))\"\n",
    "                )\n",
    "\n",
    "            final_df = combined_df[difference_columns]\n",
    "            \n",
    "    print(f\"Comparison Excel saved at {comparison_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"Upload Path\"\n",
    "    output_directory = \"Output Path\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    process_files(directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc390d8-cc88-4edf-9360-d337b3d923aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Define the path to the Excel file\n",
    "excel_file = \"Output Path/consolidated_output.xlsx\"\n",
    "output_dir = os.path.abspath(os.path.dirname(excel_file))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = {\n",
    "    \"gray\": \"#6a6a6b\",\n",
    "    \"blue\": \"#0400f9\",\n",
    "    \"seafoam\": \"#00ff80\",\n",
    "    \"bright_purple\": \"#8100fb\",\n",
    "    \"sky_blue\": \"#0A56B1\",\n",
    "    \"green\": \"#307f2f\",\n",
    "    \"brick\": \"#7f432f\",\n",
    "    \"red\": \"#ff0000\",\n",
    "    \"orange\": \"#ee6200\"\n",
    "}\n",
    "\n",
    "# Function to format y-axis\n",
    "format_thousands = lambda x, _: f'{int(x):,}'\n",
    "\n",
    "# Function to group carton types\n",
    "def group_carton_types(carton_name):\n",
    "    if \"Box\" in carton_name:\n",
    "        if \"Small\" in carton_name:\n",
    "            return \"Small Boxes\"\n",
    "        elif \"Medium\" in carton_name:\n",
    "            return \"Medium Boxes\"\n",
    "        elif \"Large\" in carton_name:\n",
    "            return \"Large Boxes\"\n",
    "    elif \"Mailer\" in carton_name:\n",
    "        return \"Mailers\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Read Excel sheets\n",
    "excel_data = pd.ExcelFile(excel_file)\n",
    "sheets = excel_data.sheet_names\n",
    "\n",
    "# Generate plots for each sheet\n",
    "for sheet in sheets:\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet)\n",
    "\n",
    "    # Group carton types\n",
    "    if 'name' in df.columns:\n",
    "        df['grouped_name'] = df['name'].apply(group_carton_types)\n",
    "    \n",
    "    # Convert data types\n",
    "    df['surface_area'] = pd.to_numeric(df['surface_area'], errors='coerce')\n",
    "    df['Carton_volume'] = pd.to_numeric(df['Carton_volume'], errors='coerce')\n",
    "\n",
    "    # Filter missing values\n",
    "    df = df.dropna(subset=['total_weight', 'Billed_Weight', 'dim_weight', 'Price', 'base_cost', 'Order_volume', 'Carton_volume'])\n",
    "\n",
    "    # Summarize data\n",
    "    summary_data = df.groupby('source_flag').agg({\n",
    "        'Billed_Weight': 'sum',\n",
    "        'dim_weight': 'sum',\n",
    "        'total_weight': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "        # Combo Chart: Dim Weight vs Total Weight\n",
    "    if 'dim_weight' in df.columns and 'total_weight' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=summary_data, x='source_flag', y='dim_weight', color= custom_colors['gray'])\n",
    "        sns.lineplot(data=summary_data, x='source_flag', y=summary_data['total_weight'] / 2, color= custom_colors['seafoam'], marker='X', markersize=15, linewidth=3)\n",
    "        plt.title('Combo Chart: Dim Weight vs Total Weight')\n",
    "        plt.xlabel('Source Name')\n",
    "        plt.ylabel('Dim Weight')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"actual_vs_dim_weight.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "        # Combo Chart: Billed Weight vs Total Weight\n",
    "    if 'Billed_Weight' in df.columns and 'total_weight' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=summary_data, x='source_flag', y='Billed_Weight', color= custom_colors['gray'])\n",
    "        sns.lineplot(data=summary_data, x='source_flag', y=summary_data['total_weight'] / 2, color= custom_colors['brick'], marker='X', markersize=15, linewidth=3)\n",
    "        plt.title('Combo Chart: Billed Weight vs Total Weight')\n",
    "        plt.xlabel('Source Name')\n",
    "        plt.ylabel('Billed Weight')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"actual_vs_billed_weight.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "        # Surface Area Chart\n",
    "    if 'surface_area' in df.columns:\n",
    "        surface_data = df.groupby('source_flag')['surface_area'].sum().reset_index()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(data=surface_data, x='source_flag', y='surface_area',  hue='source_flag', legend=False)\n",
    "        plt.title('Surface Area')\n",
    "        plt.xlabel('Source Name')\n",
    "        plt.ylabel('Total Surface Area')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"SA_agg_comp.png\"))\n",
    "        plt.close()\n",
    "\n",
    "     # Dimmed vs Count Chart\n",
    "    if 'Dimmed' in df.columns:\n",
    "        dimmed_data = df.groupby(['source_flag', 'Dimmed']).size().reset_index(name='count')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=dimmed_data, x='source_flag', y='count', hue='Dimmed')\n",
    "        plt.title('Counts of Dimmed (Yes/No) by Source Flag')\n",
    "        plt.xlabel('Source Flag')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"dimmed_vs_count.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # Dim Weight vs Billed Weight\n",
    "    if 'dim_weight' in df.columns and 'Billed_Weight' in df.columns:\n",
    "        weight_data = df.groupby('source_flag').agg({'dim_weight': 'sum', 'Billed_Weight': 'sum'}).reset_index()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=weight_data, x='source_flag', y='Billed_Weight', label='Billed Weight', color= custom_colors['gray'])\n",
    "        sns.lineplot(data=weight_data, x='source_flag', y='dim_weight', marker='X', linewidth=3, markersize=15, color= custom_colors['red'], label='Dim Weight')\n",
    "        plt.title('Sum of Dim Weight and Billed Weight by Source Flag')\n",
    "        plt.xlabel('Source Flag')\n",
    "        plt.ylabel('Total Weight')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"dim_weight_vs_billed_weight.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Orders Billed Over Actual\n",
    "    if 'Billed_over_Actual' in df.columns and 'orderId' in df.columns:\n",
    "        orders_data = df.groupby(['Billed_over_Actual', 'source_flag']).size().reset_index(name='order_count')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=orders_data, x='Billed_over_Actual', y='order_count', hue='source_flag', marker='X', linewidth=5, markersize=15)\n",
    "        plt.title('Combo Chart: Billed over Actual vs Count of Order ID by Source Flag')\n",
    "        plt.xlabel('Billed over Actual')\n",
    "        plt.ylabel('Count of Order ID')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"orders_billed_over_actual.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Price by Carton Type\n",
    "    if 'Price' in df.columns:\n",
    "        price_data = df.groupby(['name', 'source_flag']).agg({'Price': 'mean'}).reset_index()\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=price_data, x='name', y='Price', hue='source_flag')\n",
    "        plt.title('Average Price / Carton Type')\n",
    "        plt.xlabel('Carton Type')\n",
    "        plt.ylabel('Average Price')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"price_by_carton_type.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Volume Utilization by Carton Type\n",
    "    if 'volume_utilization' in df.columns:\n",
    "        volume_data = df.groupby(['name', 'source_flag']).agg({'volume_utilization': 'mean'}).reset_index()\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=volume_data, x='name', y='volume_utilization', hue='source_flag')\n",
    "        plt.legend(title='Source Flag', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "        plt.title('Average Volume Utilization / Carton Type')\n",
    "        plt.xlabel('Carton Type')\n",
    "        plt.ylabel('Average Volume Utilization')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"volume_utilization_by_carton_type.png\"))\n",
    "        plt.close()\n",
    "\n",
    "print(\"All plots have been successfully generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe7b0e-e089-4028-b61c-0d72f1162679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from tqdm import tqdm\n",
    "from PIL import Image as PILImage  \n",
    "from io import BytesIO\n",
    "\n",
    "# Function to open the image for insertion into Excel\n",
    "def open_image(image_path, max_width=1200, max_height=1000):\n",
    "    try:\n",
    "        img = PILImage.open(image_path)  # Open the image using PIL\n",
    "        img_width, img_height = img.size\n",
    "        scale = min(max_width / img_width, max_height / img_height) # Calculate the resizing scale to maintain aspect ratio\n",
    "        new_width = int(img_width * scale) # Calculate new dimensions\n",
    "        new_height = int(img_height * scale) \n",
    "        img = img.resize((new_width, new_height), PILImage.Resampling.LANCZOS) # Resize the image\n",
    "        img_byte_arr = BytesIO() # Convert the resized image to a byte stream (in memory) for use in openpyxl\n",
    "        img.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr.seek(0)  # Reset pointer to the beginning of the byte stream\n",
    "        return Image(img_byte_arr) # Return the image for insertion into Excel\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {image_path} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main processing function to load and process files\n",
    "def process_files(directory, output_directory):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    baseline_file = None\n",
    "    for filename in files:\n",
    "        if \"baseline\" in filename.lower():\n",
    "            baseline_file = filename\n",
    "            break\n",
    "\n",
    "    if not baseline_file:\n",
    "        raise ValueError(\"Baseline file not found. Ensure a file containing 'baseline' in its name exists.\")\n",
    "\n",
    "    image_directory = 'Output Path' \n",
    "    image_paths = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.endswith('.png')]\n",
    "    \n",
    "    wb = load_workbook('Output Path/combined_output.xlsx')  # Load the Excel file\n",
    "    ws_images = wb.create_sheet('Visuals')  \n",
    "    current_row = 2  # Initialize row position\n",
    "    current_column = 2\n",
    "    \n",
    "    with tqdm(total=len(image_paths), desc=\"Inserting Images into Excel\", unit=\"image\", colour=\"cyan\") as pbar:\n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            img = open_image(image_path, max_width=1200, max_height=1000)\n",
    "\n",
    "            if img:\n",
    "                cell_position = f\"{chr(64 + current_column)}{current_row}\"\n",
    "                ws_images.add_image(img, cell_position)\n",
    "                # Adjust row and column for the next image\n",
    "                current_column += 50  # Move to the next column\n",
    "                if current_column > 1:  # Adjust number of columns per row \n",
    "                    current_column = 2  # Reset to column D\n",
    "                    current_row += 50  # Move to the next row\n",
    "            else:\n",
    "                print(f\"Image {image_path} could not be opened.\")  # Debugging log\n",
    "            pbar.update(1)\n",
    "    \n",
    "    run_match = re.search(r\"filepath_(\\d+)\", baseline_file)  # Extracting run number \n",
    "    run_number = run_match.group(1)\n",
    "\n",
    "    final_filename = f\"_Output_run_{run_number}.xlsx\" # Define the new output file name\n",
    "    final_path = os.path.join(output_directory, final_filename)\n",
    "\n",
    "    wb.save(final_path)  \n",
    "    print(f\"Final file saved with visuals on a separate sheet: {final_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"Upload Path\"  \n",
    "    output_directory = \"Output Path\"  \n",
    "    process_files(directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132ad3c-223e-43cd-a98e-c341022529e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def refine_suffix(filename):\n",
    "\n",
    "    match = re.search(r\"filepath_(\\d+)\\.(.*?)(?:$|\\.)\", filename)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "print(refine_suffix(\"filepath_####.baseline.output_cartons\")) \n",
    "print(refine_suffix(\"filepath_####.fileA.output_cartons\"))\n",
    "print(refine_suffix(\"filepath_####.fileB.output_cartons\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
